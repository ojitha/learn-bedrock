{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d1591dd",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  AWS Bedrock Multi-Agent RAG with LangGraph\n",
    "date:   2025-08-29\n",
    "categories: [AI, AWS]\n",
    "mermaid: true\n",
    "typora-root-url: /Users/ojitha/GitHub/ojitha.github.io\n",
    "typora-copy-images-to: ../assets/images/${filename}\n",
    "---\n",
    "\n",
    "<style>\n",
    "/* Styles for the two-column layout */\n",
    ".image-text-container {\n",
    "    display: flex; /* Enables flexbox */\n",
    "    flex-wrap: wrap; /* Allows columns to stack on small screens */\n",
    "    gap: 10px; /* Space between the image and text */\n",
    "    align-items: left; /* Vertically centers content in columns */\n",
    "    margin-bottom: 20px; /* Space below this section */\n",
    "}\n",
    "\n",
    ".image-column {\n",
    "    flex: 1; /* Allows this column to grow */\n",
    "    min-width: 150px; /* Minimum width for the image column before stacking */\n",
    "    max-width: 20%; /* Maximum width for the image column to not take up too much space initially */\n",
    "    box-sizing: border-box; /* Include padding/border in element's total width/height */\n",
    "}\n",
    "\n",
    ".text-column {\n",
    "    flex: 2; /* Allows this column to grow more (e.g., twice as much as image-column) */\n",
    "    min-width: 300px; /* Minimum width for the text column before stacking */\n",
    "    box-sizing: border-box;\n",
    "}\n",
    "\n",
    "</style>\n",
    "\n",
    "<div class=\"image-text-container\">\n",
    "    <div class=\"image-column\">\n",
    "        <img src=\"/assets/images/2025-08-29-BedrockLangGraph/agentic_ai_logo.svg\" alt=\"Scala basics\" width=\"150\" height=\"150\">\n",
    "    </div>\n",
    "    <div class=\"text-column\">\n",
    "<p>This implementation demonstrates building Multi-Agent RAG systems using AWS Bedrock's Amazon Large Language Models (LLMs) integrated with LangGraph for intelligent document processing (IDP). The system implements document relevance grading using Titan Express LLM and integrates Chroma vector database with Amazon Titan embeddings for semantic search, processing articles into optimized chunks for RAG performance. LangGraph's workflow orchestration creates adaptive AI agents that automatically retry and transform queries up to 3 times with sophisticated conditional routing logic based on document relevance scores. The multi-agent architecture utilizes specialized Bedrock models for answer generation, document grading, and query rewriting, creating a production-ready system with LangChain Expression Language (LCEL) integration for seamless component chaining.</p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<!--more-->\n",
    "\n",
    "------\n",
    "\n",
    "* TOC\n",
    "{:toc}\n",
    "------\n",
    "\n",
    "## Introduction\n",
    "\n",
    "A framework LangChain[^1] is for building applications powered by Large Language Models (LLM). According to the LangChain, LangGraph to create Multi AI Agent systems who uses an LLM to decide the control flow of an application: but there is no exact difition. Instead LangChain introduce the different levels of autonomy:\n",
    "\n",
    "![Autonomy of AI Agents](https://blog.langchain.com/content/images/2024/06/Screenshot-2024-06-28-at-7.33.10-PM.png){:width=\"50%\" height=\"50%\"}\n",
    "\n",
    "\n",
    "> LangGraph, the agent orchestrator to help with building, running, and interacting with agents, and LangSmith, the testing and observability platform for LLM apps.\n",
    "{:.green}\n",
    "\n",
    "In the above spectrum, typically LangGraph can be used to develop Agents for the levels 4,5 and 6 where:\n",
    "\n",
    "- Router: LLM routes inputs into specific downstream workflows.\n",
    "- State Machine: LLMs determine whether to continue base on the state.\n",
    "- Autonomous: System build tools, remembers them and uses them in future steps.\n",
    "\n",
    "> Router level is high application reliability but at autonomus level the lowest application reliability.\n",
    "{:.yellow}\n",
    "\n",
    "I demonstrates how to build sophisticated Multi-Agent RAG (Retrieval Augmented Generation) systems using AWS Bedrock's Amazon Titan and Nova Pro models integrated with LangGraph for IDP (intelligent document processing). The system implements advanced document relevance grading using Amazon Titan Express LLM with structured outputs and Pydantic models, enabling intelligent filtering of retrieved documents based on semantic similarity and contextual relevance to user queries. \n",
    "\n",
    "AWS CLI command to list Bedrock models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5eb69fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon.titan-text-lite-v1:0:4k\n",
      "amazon.titan-text-lite-v1\n",
      "amazon.titan-text-express-v1:0:8k\n",
      "amazon.titan-text-express-v1\n",
      "amazon.titan-embed-image-v1:0\n",
      "amazon.titan-embed-image-v1\n",
      "amazon.titan-embed-text-v2:0\n",
      "amazon.nova-pro-v1:0\n",
      "amazon.nova-lite-v1:0\n",
      "amazon.nova-micro-v1:0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "aws bedrock list-foundation-models \\\n",
    "    --by-provider \"Amazon\" \\\n",
    "    --output json | \\\n",
    "    jq -r '.modelSummaries[] | select(.modelLifecycle.status == \"ACTIVE\" ) |.modelId'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1148c4e3",
   "metadata": {},
   "source": [
    "A multi-model AWS Bedrock setup utilizes Amazon Titan Lite for answer generation, Titan Express for document grading, and Nova Pro for advanced query rewriting and optimization tasks, showcasing the power of specialized model selection.\n",
    "\n",
    "LangGraph's state-based workflow orchestration creates adaptive AI agents that automatically retry and transform queries up to 3 times when initial document retrievals fail to meet relevance thresholds, ensuring robust error handling and query optimization. This Multi-Agent architecture[^2] integrates Chroma vector database with Amazon Titan embeddings for document search, processing agent-related articles into 250-token chunks for optimal retrieval performance in RAG applications. \n",
    "\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    subgraph \"üéØ Input Layer\"\n",
    "        INPUT[\"üë§ User Question<br/>e.g. What is Deep learning?\"]\n",
    "    end\n",
    "    \n",
    "    subgraph \"‚öôÔ∏è LangGraph Workflow\"\n",
    "        INIT[\"üöÄ set_state<br/>Initialize State<br/>times_transformed = 0\"]\n",
    "        \n",
    "        RETRIEVE[\"üîç retrieve<br/>Vector Search<br/>üìä Chroma DB + Titan Embeddings\"]\n",
    "        \n",
    "        GRADE[\"‚úÖ grade_documents<br/>Document Relevance Grading<br/>ü§ñ Titan Express LLM\"]\n",
    "        \n",
    "        DECISION{\"ü§î decide_to_generate<br/>Conditional Logic<br/>Check relevance & retry count\"}\n",
    "        \n",
    "        TRANSFORM[\"üîÑ transform_query<br/>Query Rewriting<br/>üß† Nova Pro LLM\"]\n",
    "        \n",
    "        GENERATE[\"üìù generate<br/>Answer Generation<br/>‚ö° Titan Lite LLM + RAG Chain\"]\n",
    "    end\n",
    "    \n",
    "    subgraph \"üíæ Data Sources\"\n",
    "        VECTOR[(\"üóÑÔ∏è Vector Store<br/>üìä Chroma DB<br/>Agent-related Articles\")]\n",
    "        EMBEDDINGS[\"üîó Amazon Titan<br/>üìê Embedding Model<br/>Text to Vectors\"]\n",
    "    end\n",
    "    \n",
    "    subgraph \"ü§ñ AWS AI Models\"\n",
    "        TITAN_EXPRESS[\"‚ö° Amazon Titan Express<br/>üìã Document Grading\"]\n",
    "        NOVA_PRO[\"üß† Amazon Nova Pro<br/>‚úèÔ∏è Query Rewriting\"]\n",
    "        TITAN_LITE[\"üí¨ Amazon Titan Lite<br/>üìñ Answer Generation\"]\n",
    "    end\n",
    "    \n",
    "    subgraph \"üì§ Output Layer\"\n",
    "        ANSWER[\"‚ú® Generated Answer<br/>üìÑ Contextual Response\"]\n",
    "        FAILURE[\"‚ùå Fallback Answer<br/>ü§∑ I don't know response\"]\n",
    "    end\n",
    "    \n",
    "    INPUT --> INIT\n",
    "    INIT --> RETRIEVE\n",
    "    RETRIEVE --> GRADE\n",
    "    GRADE --> DECISION\n",
    "    \n",
    "    DECISION -->|\"‚ùå No relevant docs<br/>times_transformed < 3\"| TRANSFORM\n",
    "    DECISION -->|\"‚úÖ Has relevant docs<br/>OR retry limit reached\"| GENERATE\n",
    "    \n",
    "    TRANSFORM --> RETRIEVE\n",
    "    GENERATE --> ANSWER\n",
    "    GENERATE --> FAILURE\n",
    "    \n",
    "    RETRIEVE -.-> VECTOR\n",
    "    VECTOR -.-> EMBEDDINGS\n",
    "    GRADE -.-> TITAN_EXPRESS\n",
    "    TRANSFORM -.-> NOVA_PRO\n",
    "    GENERATE -.-> TITAN_LITE\n",
    "\n",
    "    %% Styling\n",
    "    classDef inputStyle fill:#FF9900,stroke:#FF6600,stroke-width:3px,color:#fff\n",
    "    classDef workflowStyle fill:#232F3E,stroke:#FF9900,stroke-width:2px,color:#fff\n",
    "    classDef storageStyle fill:#3498db,stroke:#2980b9,stroke-width:2px,color:#fff\n",
    "    classDef modelStyle fill:#e74c3c,stroke:#c0392b,stroke-width:2px,color:#fff\n",
    "    classDef outputStyle fill:#27ae60,stroke:#229954,stroke-width:2px,color:#fff\n",
    "    classDef decisionStyle fill:#f39c12,stroke:#e67e22,stroke-width:2px,color:#fff\n",
    "\n",
    "    class INPUT inputStyle\n",
    "    class INIT,RETRIEVE,GRADE,TRANSFORM,GENERATE workflowStyle\n",
    "    class VECTOR,EMBEDDINGS storageStyle\n",
    "    class TITAN_EXPRESS,NOVA_PRO,TITAN_LITE modelStyle\n",
    "    class ANSWER,FAILURE outputStyle\n",
    "    class DECISION decisionStyle\n",
    "```\n",
    "\n",
    "The system implements sophisticated conditional routing logic with LangGraph's decision nodes, enabling dynamic workflow paths based on document relevance scores and transformation attempt counters. Comprehensive LangChain Expression Language (LCEL) integration builds declarative AI pipelines with streaming support, async operations, and seamless component chaining across multiple AWS Bedrock models. \n",
    "\n",
    "The complete RAG data preparation pipeline uses WebBaseLoader for document ingestion, `RecursiveCharacterTextSplitter` for intelligent text chunking, and tiktoken-based token counting for LLM context optimization, creating a production-ready multi-agent system for IDP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c0038e",
   "metadata": {},
   "source": [
    "### Community package\n",
    "Langchain Community[^7] that contains third-party integrations and community-contributed components for the LangChain ecosystem. It serves as the integration layer between LangChain's core functionality and external services, tools, and platforms.\n",
    "\n",
    "> Before use install the package `pip install langchain-community`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99065aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Intelligently splits documents while preserving context\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Loads documents from web URLs\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Vector database for similarity search\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# AWS Bedrock embedding model integration\n",
    "from langchain_aws import BedrockEmbeddings \n",
    "\n",
    "\n",
    "boto3.set_stream_logger('', logging.ERROR)\n",
    "\n",
    "# Initialize AWS Bedrock client\n",
    "bedrock_client = boto3.client(\n",
    "    service_name='bedrock-runtime'\n",
    "    ,region_name='ap-southeast-2'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f56dca",
   "metadata": {},
   "source": [
    "Create Bedrock embeddings instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e39f4ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = BedrockEmbeddings(\n",
    "    client=bedrock_client,\n",
    "    model_id=\"amazon.titan-embed-text-v1\"  # or use v2 for better performance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156406d",
   "metadata": {},
   "source": [
    "## Embeddings pipeline\n",
    "*Embeddings* are numerical representations of text, where similar pieces of text are converted into similar vectors (arrays of numbers). Think of them as coordinates in a multi-dimensional space where semantically related content clusters together.\n",
    "\n",
    "How Embeddings Capture Meaning\n",
    "\n",
    "- *Semantic similarity*: Words/phrases with similar meanings have similar vectors\n",
    "- *Context awareness*: \"bank\" (river) vs \"bank\" (financial) get different embeddings\n",
    "- *Dimensional encoding*: Each dimension captures different aspects of meaning\n",
    "\n",
    "What is need to do:\n",
    "\n",
    "1. Load documents from URLs\n",
    "2. Split into chunks (250 tokens each)\n",
    "3. Each chunk becomes a separate document\n",
    "4. Amazon Titan reads each text chunk\n",
    "5. Converts text into a 1536-dimensional vector\n",
    "6. Each dimension represents learned semantic features\n",
    "\n",
    "### Chroma for RAG\n",
    "Chroma is an open-source AI-native vector database designed specifically for building AI applications with embeddings. It's particularly popular in the RAG (Retrieval Augmented Generation) ecosystem and integrates seamlessly with LangChain, as demonstrated in your notebook. Chroma is a vector database that:\n",
    "\n",
    "- Stores embeddings (numerical representations of text/data)\n",
    "- Performs semantic similarity search using vector mathematics\n",
    "- Provides metadata filtering and hybrid search capabilities\n",
    "- Offers both in-memory and persistent storage options\n",
    "- Integrates natively with AI/ML frameworks\n",
    "\n",
    "Here the complete RAG data preparation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69ddab9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tiktoken-based text splitter\n",
      "Created 187 document chunks\n"
     ]
    }
   ],
   "source": [
    "# Initialize Bedrock embeddings with Amazon Titan model\n",
    "embeddings = BedrockEmbeddings(\n",
    "    client=bedrock_client,\n",
    "    model_id=\"amazon.titan-embed-image-v1\"  # Using Amazon Titan embedding model\n",
    ")\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "\n",
    "# Load documents from URLs\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Option 1: Use tiktoken-based splitter (if tiktoken is installed)\n",
    "try:\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=250, chunk_overlap=0\n",
    "    )\n",
    "    print(\"Using tiktoken-based text splitter\")\n",
    "except ImportError:\n",
    "    # Option 2: Fallback to character-based splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=250,\n",
    "        chunk_overlap=0,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    print(\"Using character-based text splitter (tiktoken not available)\")\n",
    "\n",
    "# Split documents into chunks\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "print(f\"Created {len(doc_splits)} document chunks\")\n",
    "\n",
    "# Add to vectorDB with Bedrock embeddings\n",
    "vectorstore = Chroma.from_documents( # Creates vector store from document chunks\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma-bedrock\",  # With collection name (from your notebook)\n",
    "    embedding=embeddings,  # Using Bedrock embeddings\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84201305",
   "metadata": {},
   "source": [
    "In the above code `RecursiveCharacterTextSplitter.from_tiktoken_encoder()` is the *Token-aware* text splitting for LLM context limits. \n",
    "It used the following Parameters:\n",
    "\n",
    "- chunk_size=250: Maximum tokens per chunk\n",
    "- chunk_overlap=0: No overlap between chunks\n",
    "\n",
    "Creates vector store from document chunks: That will automatically embeds all documents and stores in vector DB. The last statement of the above code creates retriever interface for similarity search which returns `top-k` similar documents for queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061c3823",
   "metadata": {},
   "source": [
    "Test document retrieval functionality. Test the vector database contents. It should return `List[Document]` with similarity-ranked results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51e33491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short-Term Memory (STM) or Working Memory: It stor Ellipsis https://lilianweng.github.io/posts/2023-06-23-agent/\n",
      "Short-term memory: I would consider all the in-con Ellipsis https://lilianweng.github.io/posts/2023-06-23-agent/\n",
      "Sensory memory as learning embedding representatio Ellipsis https://lilianweng.github.io/posts/2023-06-23-agent/\n",
      "Constraints:\n",
      "1. ~4000 word limit for short term me Ellipsis https://lilianweng.github.io/posts/2023-06-23-agent/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8w/ytv2_9yd0nld4gbqjwbs4yw40000gn/T/ipykernel_46686/3939294398.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents (question)\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Short-Term Memory?\"\n",
    "docs = retriever.get_relevant_documents (question)\n",
    "for doc in docs:\n",
    "    print (doc.page_content[:50], ... , doc.metadata['source'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e50f60",
   "metadata": {},
   "source": [
    "## Retrieval Grader Implementation\n",
    "\n",
    "Grader is a document relevance assessment using structured outputs. For that this code using `ChatPromptTemplate` and `ChatBedrock`. In this case `BaseModel` is from *Pydantic*[^5].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5607e4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ojitha/workspace/learn-bedrock/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3699: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "### Simple Direct Conversion: Retrieval Grader with AWS Bedrock Titan\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "import re\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    client=bedrock_client,\n",
    "    model_id=\"amazon.titan-text-express-v1\",  \n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,  # Same temperature as original\n",
    "        \"maxTokenCount\": 1000\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623752c0",
   "metadata": {},
   "source": [
    "### Structured output\n",
    "\n",
    "The function `structured_llm_grader` is the Custom Structured Output Handler. Structured outputs of an LLM invocation facilitate seamless interaction between different components and agents. Agents communicate in multi-step workflows, requiring consistent and interpretable outputs.\n",
    "\n",
    "Formats for Structured Output:\n",
    "\n",
    "- JSON: Widely used for its readability and compatibility.\n",
    "- Pydantic Models: Provides data validation and type enforcement in Python.\n",
    "\n",
    "The `structured_llm_grader(messages)` function in the notebook represents a custom *workaround implementation* for achieving structured outputs with AWS Bedrock Titan models (Standard approach with supported models e.g., OpenAI), which don't natively support LangChain's standard `with_structured_output()` method.\n",
    "\n",
    "Strategy 1: JSON Pattern Extraction\n",
    "\n",
    "- Purpose: Attempts to parse structured JSON from free-form text responses\n",
    "- Regex Pattern: `r'\\{[^}]*\"binary_score\"[^}]*\\}'` searches for JSON objects containing \"binary_score\"\n",
    "- Parsing Logic: Extracts and parses the JSON, then constructs a `GradeDocuments` Pydantic object\n",
    "- Reliability: Moderate - depends on the LLM consistently outputting valid JSON\n",
    "\n",
    "Strategy 2: Keyword-Based Fallback\n",
    "\n",
    "- Purpose: Provides robust fallback when JSON parsing fails\n",
    "- Logic: Simple boolean logic checking for presence of \"yes\" without \"no\"\n",
    "- Default Behavior: Assumes \"no\" (conservative approach for document relevance)\n",
    "- Reliability: High - always produces a result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae898895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom structured output handler (since Titan doesn't support with_structured_output directly)\n",
    "def structured_llm_grader(messages):\n",
    "    \"\"\"Custom function to handle structured output from Titan model\"\"\"\n",
    "    response = llm.invoke(messages)\n",
    "    response_text = response.content\n",
    "    \n",
    "    # Try to parse JSON response\n",
    "    try:\n",
    "        # Strategy 1: JSON Pattern Matching\n",
    "        json_match = re.search(r'\\{[^}]*\"binary_score\"[^}]*\\}', response_text, re.IGNORECASE)\n",
    "        if json_match:\n",
    "            json_data = json.loads(json_match.group())\n",
    "            return GradeDocuments(binary_score=json_data.get('binary_score', 'no'))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Strategy 2: Keyword Fallback\n",
    "    if 'yes' in response_text.lower() and 'no' not in response_text.lower():\n",
    "        return GradeDocuments(binary_score='yes')\n",
    "    else:\n",
    "        return GradeDocuments(binary_score='no')\n",
    "\n",
    "# Enhanced prompt with JSON format requirement\n",
    "system = \"\"\"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b475e7",
   "metadata": {},
   "source": [
    "### Prompt templates\n",
    "Here the list of common LangChain template patterns:\n",
    "\n",
    "| Pattern              | Use Case                       | Template Type                                 |\n",
    "| -------------------- | ------------------------------ | --------------------------------------------- |\n",
    "| **Classification**   | Binary/multi-class decisions   | `ChatPromptTemplate` with system instructions |\n",
    "| **Generation**       | Content creation               | `PromptTemplate` with context variables       |\n",
    "| **Transformation**   | Query rewriting, translation   | `ChatPromptTemplate` with examples            |\n",
    "| **Extraction**       | Information parsing            | `PromptTemplate` with output parser           |\n",
    "| **RAG**              | Retrieval-augmented generation | Hub templates or custom with context          |\n",
    "| **Chain-of-Thought** | Step-by-step reasoning         | `FewShotPromptTemplate` with examples         |\n",
    "| **Function Calling** | Structured outputs             | `ChatPromptTemplate` with JSON schema         |\n",
    "\n",
    "The `ChatPromptTemplate` is the most often used for AI Agents for the  conversational models requiring structured message formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16720b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "])\n",
    "\n",
    "# Create the retrieval grader function\n",
    "def retrieval_grader(inputs):\n",
    "    \"\"\"Grade document relevance using Bedrock Titan model\"\"\"\n",
    "    messages = grade_prompt.format_prompt(**inputs).to_messages()\n",
    "    return structured_llm_grader(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5145b69",
   "metadata": {},
   "source": [
    "It is important to notice that retrieval_grader(inputs) is the grader function. If you have a retriever from the embeddings code.\n",
    "\n",
    "Testing retriver:\n",
    "\n",
    "1. Retrieve documents for question\n",
    "2. Grade each document for relevance\n",
    "3. Return structured GradeDocuments object\n",
    "4. Print binary scores (\"yes\"/\"no\")\n",
    "\n",
    "In the following code, behind the scenes:\n",
    "1. Question ‚Üí Amazon Titan ‚Üí embedding vector (for the question)\n",
    "2. Chroma searches for similar document vectors (for the qestion created above) \n",
    "3. Returns top-k most similar document chunks\n",
    "4. Each document has metadata (source URL, chunk info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd54f43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n",
      "binary_score='yes'\n",
      "binary_score='no'\n",
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "# Test example (same as original structure)\n",
    "question = \"Short-Term Memory\"\n",
    "\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "for doc in docs:\n",
    "    doc_txt = doc.page_content\n",
    "    result = retrieval_grader({\"question\": question, \"document\": doc_txt})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46ba8e3",
   "metadata": {},
   "source": [
    "Build generation pipeline using LangChain Hub which will download the prebuild prompts. For example, **rlm/rag-prompt** is Standard RAG question-answering template. \n",
    "\n",
    "The hub prompt contains:\n",
    "\n",
    "1. Role definition: \"assistant for question-answering tasks\"\n",
    "2. Context injection: Uses retrieved documents\n",
    "3. Constraints: \"three sentences maximum\", \"say you don't know if uncertain\"\n",
    "4. Template variables: `{question}` and `{context}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af28122b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ojitha/workspace/learn-bedrock/.venv/lib/python3.13/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.chat.HumanMessagePromptTemplate'>\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: {question} \n",
      "Context: {context} \n",
      "Answer:\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "for message in prompt.messages:\n",
    "    print(type(message))\n",
    "    print(message.prompt.template)\n",
    "    print('-----')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0395d1",
   "metadata": {},
   "source": [
    "### LCEL\n",
    "In the following code LangCain Expression Langauge (LCEL)[^6] to build the declarative way to chain LangChain components. In this case we use sequential composition of components (prompts, models, parsers). But LCEL is capable of:\n",
    "- Streaming support\n",
    "- Async operations\n",
    "- Parallel execution\n",
    "- Retries and fallbacks\n",
    "- Access to intermediate results\n",
    "\n",
    "\n",
    "> The `StrOutputParser` is optiona, but better extracts string content from LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ef9e6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Short-term memory is believed to have the capacity of about 7 items and lasts for 20-30 seconds.\n"
     ]
    }
   ],
   "source": [
    "# This time LLM is Amazon Titan Lite\n",
    "llm_another = ChatBedrock(\n",
    "    client=bedrock_client,\n",
    "    model_id=\"amazon.titan-text-lite-v1\",  \n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,  # Same temperature as original\n",
    "        \"maxTokenCount\": 1000\n",
    "    }\n",
    ")\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# LCEL Chain\n",
    "rag_chain = prompt | llm_another | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": format_docs(docs), \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ced183",
   "metadata": {},
   "source": [
    "### Query optimization using advanced LLM\n",
    "\n",
    "Section for query rewriting with more capable model such as Amazon Nova Pro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fe414b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A different LLM, just to show we can use multiple LLMs in our calls\n",
    "bigger_llm = ChatBedrock(\n",
    "    client=bedrock_client,\n",
    "    model_id=\"amazon.nova-pro-v1:0\",  \n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,  # Same temperature as original\n",
    "        \"maxTokenCount\": 1000\n",
    "    }\n",
    ")\n",
    "# Prompt\n",
    "system = \"\"\"You are a question re-writer that converts an input question to a better version that is optimized \\n\n",
    "     for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750d6fac",
   "metadata": {},
   "source": [
    "### RAG Chain\n",
    "\n",
    "Using the LCLE language RAG chain can be created where \n",
    "\n",
    "- Sequential processing pipeline (Prompt Template formats input with context and question)\n",
    "- Each component feeds into the next (Titan Lite LLM generates answer from formatted prompt)\n",
    "- Enables functional composition of LLM operations (Output Parser extracts clean text response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a04c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCLE RAG chain\n",
    "question_rewriter = re_write_prompt | bigger_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d9b13a",
   "metadata": {},
   "source": [
    "## Single Agent vs Multi-agents\n",
    "\n",
    "### ReAct pattern\n",
    "Although Chain-of-Thourght is most common, recently ReAct pattern is started adopt where reasoning (out performed the zero-shot) and acting can be more efficent and avoid hallucinations. A framework where an agent alternates between thinking (reasoning) and doing (acting) to solve tasks. ReAct is the pattern implemented for single agent. \n",
    "\n",
    "- The agent writes a thought about the task.\n",
    "- Based on that thought, it performs an action.\n",
    "- Observes the output and repeats the cycle until the task is complete.\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    T --> A[Action]\n",
    "\tO[Observation] --> T[Thought]\n",
    "    A --> O\n",
    "```\n",
    "\n",
    "> In the ReAct, Agent start with the Thought and loop in number of cycles. There is a limit of maximum cycles to be reached. In the ReAct loop exit is a challange. If there is no limits for the cycles, then complex task takes more time.\n",
    "{:.yellow}\n",
    "\n",
    "Single Agent systems are suffering with the following problems:\n",
    "\n",
    "- Not easy to handle complexity: Struggles with multi-step reasoning and long sequences of actions.\n",
    "- Limited Tools handling: Struggles with handling a large number of tools'\n",
    "- No efficent parallelism: Cannot handle parallel tasks efficiently.\n",
    "\n",
    "### Multi-Agent\n",
    "Solution to the above problem is multi-agent systems but designing and coordinating multiple agents requires more complex infrastructure, and orchestration such as LangGraph[^3]. However, multi-agents are capable of handling complex agent orchestrations such as interactive knowledge curation:\n",
    "\n",
    "![](https://storm.genie.stanford.edu/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fconcept_image.1317b2dc.jpg&w=3840&q=75){:width=\"50%\" height=\"50%\"}\n",
    "\n",
    "source: Project STORM[^4] - \"Synthesis of Topic Outlines through Retrieval and Multi-perspective\". \n",
    "\n",
    "## Build the Graph\n",
    "\n",
    "Define shared state structure for LangGraph which is build on LangChain. LangGraph is capable of models agentic workflows as graphs and provid mechanisms for controllability, cycles, and persistence. Graph-Based Modelling includes:\n",
    "\n",
    "- **State** is the current snapshot of the workflow. Ensures consistency in data flow and throughout the\n",
    "graph.\n",
    "- **Nodes** represent actions or steps in the workflow.\n",
    "- **Edges** define the flow between nodes.\n",
    "\n",
    "LangGraph represents agent workflows as graphs. Nodes and Edges depict the flow of control and data.\n",
    "For example, we are going to buid the following LangGraph:\n",
    "\n",
    "![Example LangGraph to build](/assets/images/2025-08-29-BedrockLangGraph/Example_LangGraph_to_build.png)\n",
    "\n",
    "In shown in the above Grap:\n",
    "\n",
    "- Cycles and Branching\n",
    "    - Implement loops and conditionals within the graph.\n",
    "    - Enables agents to handle repetitive tasks and make decisions based on conditions.\n",
    "- Persistence\n",
    "    - Mediate and save state after each step in the graph.\n",
    "    - Allows to either pause or resume execution at any point.\n",
    "- Supports other features such as:\n",
    "    - Error Recovery (not in the above graph)\n",
    "    - Human-in-the-Loop Workflows (not in the above graph)\n",
    "    - Time Travel (not in the above graph)  \n",
    "\n",
    "In addition to that LangChanin support Streaming:\n",
    "\n",
    "- Stream outputs as they are produced by each node.\n",
    "- Supports token streaming for real-time data handling.    \n",
    "\n",
    "### State\n",
    "State management Pattern is as follows:\n",
    "\n",
    "- Shared state persists across all graph nodes.\n",
    "- Each node can read and update state.\n",
    "- Type hints ensure consistency.\n",
    "- Enables implementation of effective Human-Al collaboration.\n",
    "- Allows for monitoring agent progress.\n",
    "- Helps with debugging and testing workflows effectively.\n",
    "\n",
    "State Schema defines the structure of the state using:\n",
    "1. TypedDict: Sketches a type-checked dictionary structure.\n",
    "2. Pydantic BaseModel: Enriches the schema by adding validation and default values.\n",
    "\n",
    "> In the following example, we are using the option 1.\n",
    "\n",
    "Reducers determine how updates are applied to the state. Each key in the state has its own independent reducer function. There are currently two reduces\n",
    "1. **Default reducer** which is better for simple workflows such as the following example\n",
    "2. **Annotated reducer** which is better for custom function for reducer\n",
    "\n",
    "In the LangGrapy we use Python `TypeDict` to define the state and state structure with more control. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d21cc693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "        times_transformed: number of times the question has been re-written\n",
    "        web_search: if we should be doing a web search (not implemented in this notebook)\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]\n",
    "    times_transformed: int\n",
    "    web_search: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23437df",
   "metadata": {},
   "source": [
    "### Nodes\n",
    "Define LangGraph workflow nodes\n",
    "\n",
    "- `set_state(state)`\n",
    "    - **Purpose:** Initialize graph state\n",
    "    - **Returns:** `{\"times_transformed\": 0}`\n",
    "    - **Pattern:** Setup/initialization node\n",
    "\n",
    "- `retrieve(state)`  \n",
    "    - **Purpose:** Document retrieval from vector store\n",
    "    - **LangChain API:** `retriever.get_relevant_documents(question)`\n",
    "    - **Returns:** `{\"documents\": documents}`\n",
    "    - **State Dependency:** Reads `question` from state\n",
    "\n",
    "- `generate(state)`\n",
    "    - **Purpose:** Final answer generation\n",
    "    - **LangChain API:** `rag_chain.invoke()`  \n",
    "    - **Dependencies:** Uses `question` and `documents` from state\n",
    "    - **Returns:** `{\"generation\": generation}`\n",
    "\n",
    "- `transform_query(state)`\n",
    "    - **Purpose:** Query optimization/rewriting\n",
    "    - **LangChain API:** `question_rewriter.invoke()`\n",
    "    - **State Updates:** Increments `times_transformed`, updates `question`\n",
    "    - **Retry Logic:** Tracks transformation attempts\n",
    "\n",
    "- `grade_documents(state)`\n",
    "    - **Purpose:** Document relevance filtering\n",
    "    - **LangChain API:** `retrieval_grader()` for each document\n",
    "    - **Logic:** \n",
    "        - Filters relevant documents\n",
    "        - Sets `web_search=\"Yes\"` if no relevant docs\n",
    "        - **Returns:** `{\"documents\": filtered_docs, \"web_search\": web_search}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "059081b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "def set_state(state):\n",
    "    \"\"\"\n",
    "    Sets initial state\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---SET STATE---\")\n",
    "\n",
    "    return {\"times_transformed\": 0}\n",
    "\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    print(state)\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": format_docs(documents), \"question\": question})\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    times_transformed = state[\"times_transformed\"]\n",
    "    times_transformed += 1\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    print('---NEW QUESTION---')\n",
    "    print(better_question)\n",
    "    return {\"question\": better_question, \"times_transformed\": times_transformed}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        print(d.metadata['source'], f'Grade: {grade}')\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "    if len(filtered_docs) == 0:\n",
    "        print(\"---GRADE: DOCUMENTS NOT RELEVANT---\")\n",
    "        web_search = \"Yes\"\n",
    "    return {\"documents\": filtered_docs, \"web_search\": web_search}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f6eb5",
   "metadata": {},
   "source": [
    "### LangGraph Edge Pattern\n",
    "\n",
    "**Decision Logic:**\n",
    "1. **If `web_search == \"Yes\"`:**\n",
    "   - Check retry limit (`times_transformed >= 3`)\n",
    "   - Route to \"should_generate\" (give up) or \"should_transform_query\" (retry)\n",
    "2. **If `web_search == \"No\"`:**\n",
    "   - Route to \"should_generate\" (proceed with relevant docs)\n",
    "\n",
    "**Retry Pattern:**\n",
    "- Prevents infinite loops with transformation limit\n",
    "- Graceful degradation after 3 attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f87bb96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    web_search = state[\"web_search\"]\n",
    "    # state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # check times_transformed\n",
    "        if state[\"times_transformed\"] >= 3:\n",
    "            print(\n",
    "                \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION AND WE HAVE TRANSFORMED 3 TIMES, GENERATE---\"\n",
    "            )\n",
    "            return \"should_generate\"\n",
    "\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"should_transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"should_generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5bc0ff",
   "metadata": {},
   "source": [
    "Creates executable graph application which is a runnable graph with `.stream()` and `.invoke()` methods.\n",
    "\n",
    "Nodes are added to the graph using the `add_node` method where pass the state as second argument to a node after the node id. \n",
    "\n",
    "There are 2 special nodes:\n",
    "1. START Node: Entry point of the graph where user input is received.\n",
    "2. END Node: Terminal point of the graph indicating completion. \n",
    "\n",
    "There are 3 types of edges:\n",
    "1. Strating Edge: This is the edge that connects the start of the graph to a particular node. \n",
    "2. Normal Edges: Direct transitions from one node to the next.\n",
    "3. Conditional Edges: Route to different nodes based on evaluation of the state.\n",
    "4. Conditional Entry Points: This calls a function to determine which node(s) to call first when user input arrives.\n",
    "5. Ending Edge: These connect nodes to the special END node to terminate graph execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "057cd5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"set_state\", set_state)  # set_state\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"set_state\") # [1] Strating Edge \n",
    "workflow.add_edge(\"set_state\", \"retrieve\") \n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\") # [2] Normal Edge\n",
    "workflow.add_conditional_edges( # [3] Conditional Edges\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"should_transform_query\": \"transform_query\",\n",
    "        \"should_generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_edge(\"generate\", END) # [5] Ending Edge\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4406de9d",
   "metadata": {},
   "source": [
    "Here the above compiled graph:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    START([START]) --> set_state[set_state<br/>Initialize counters]\n",
    "    \n",
    "    set_state --> retrieve[retrieve<br/>Get documents from vector store]\n",
    "    \n",
    "    retrieve --> grade_documents[grade_documents<br/>Filter relevant documents]\n",
    "    \n",
    "    grade_documents --> decision{decide_to_generate<br/>Check web_search flag}\n",
    "    \n",
    "    decision -->|should_transform_query<br/>No relevant docs| transform_query[transform_query<br/>Rewrite question]\n",
    "    \n",
    "    decision -->|should_generate<br/>Has relevant docs| generate[generate<br/>Create final answer]\n",
    "    \n",
    "    transform_query --> retrieve\n",
    "    \n",
    "    generate --> END([END])\n",
    "    \n",
    "    %% Styling\n",
    "    classDef startEnd fill:#e1f5fe\n",
    "    classDef node fill:#f3e5f5\n",
    "    classDef decision fill:#fff3e0\n",
    "    classDef executed fill:#c8e6c9\n",
    "    classDef notExecuted fill:#ffcdd2\n",
    "    \n",
    "    class START,END startEnd\n",
    "    class set_state,retrieve,grade_documents,generate executed\n",
    "    class transform_query notExecuted\n",
    "    class decision decision\n",
    "```\n",
    "\n",
    "### Run the graph\n",
    "Runs the complete LangGraph workflow, stream execution with intermediate results.\n",
    "\n",
    "#### Senario: No documents returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32a28677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---SET STATE---\n",
      "Node 'set_state':\n",
      "{'question': 'What is Deep learning?', 'times_transformed': 0}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: no\n",
      "---GRADE: DOCUMENTS NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "Node 'grade_documents':\n",
      "---TRANSFORM QUERY---\n",
      "---NEW QUESTION---\n",
      "Certainly! To optimize the question \"What is Deep Learning?\" for web search, it's important to consider the specific aspects or contexts that the searcher might be interested in. Here are a few improved versions of the question that could yield more targeted and useful results:\n",
      "\n",
      "1. **General Definition:**\n",
      "   - \"What is the definition of deep learning in artificial intelligence?\"\n",
      "\n",
      "2. **Comparison with Other Techniques:**\n",
      "   - \"How does deep learning differ from traditional machine learning algorithms?\"\n",
      "\n",
      "3. **Applications:**\n",
      "   - \"What are the common applications of deep learning in various industries?\"\n",
      "\n",
      "4. **Technical Aspects:**\n",
      "   - \"What are the key components and architectures used in deep learning models?\"\n",
      "\n",
      "5. **Historical Context:**\n",
      "   - \"What is the history and evolution of deep learning technology?\"\n",
      "\n",
      "6. **Learning Resources:**\n",
      "   - \"Where can I find comprehensive resources to learn about deep learning?\"\n",
      "\n",
      "7. **Current Trends:**\n",
      "   - \"What are the latest trends and advancements in deep learning as of 2023?\"\n",
      "\n",
      "8. **Practical Implementation:**\n",
      "   - \"How can deep learning be implemented in a practical project?\"\n",
      "\n",
      "By specifying the context or the particular aspect of deep learning you're interested in, you're more likely to get relevant and high-quality search results.\n",
      "Node 'transform_query':\n",
      "{'question': 'Certainly! To optimize the question \"What is Deep Learning?\" for web search, it\\'s important to consider the specific aspects or contexts that the searcher might be interested in. Here are a few improved versions of the question that could yield more targeted and useful results:\\n\\n1. **General Definition:**\\n   - \"What is the definition of deep learning in artificial intelligence?\"\\n\\n2. **Comparison with Other Techniques:**\\n   - \"How does deep learning differ from traditional machine learning algorithms?\"\\n\\n3. **Applications:**\\n   - \"What are the common applications of deep learning in various industries?\"\\n\\n4. **Technical Aspects:**\\n   - \"What are the key components and architectures used in deep learning models?\"\\n\\n5. **Historical Context:**\\n   - \"What is the history and evolution of deep learning technology?\"\\n\\n6. **Learning Resources:**\\n   - \"Where can I find comprehensive resources to learn about deep learning?\"\\n\\n7. **Current Trends:**\\n   - \"What are the latest trends and advancements in deep learning as of 2023?\"\\n\\n8. **Practical Implementation:**\\n   - \"How can deep learning be implemented in a practical project?\"\\n\\nBy specifying the context or the particular aspect of deep learning you\\'re interested in, you\\'re more likely to get relevant and high-quality search results.', 'documents': [], 'times_transformed': 1, 'web_search': 'Yes'}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: no\n",
      "---GRADE: DOCUMENTS NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "Node 'grade_documents':\n",
      "---TRANSFORM QUERY---\n",
      "---NEW QUESTION---\n",
      "Certainly! To optimize the question \"What is Deep Learning?\" for web search, let's consider the various aspects and contexts a searcher might be interested in. Here are several refined versions of the question that can yield more targeted and useful results:\n",
      "\n",
      "1. **General Definition:**\n",
      "   - \"What is the definition of deep learning in the field of artificial intelligence?\"\n",
      "\n",
      "2. **Comparison with Other Techniques:**\n",
      "   - \"How does deep learning differ from traditional machine learning and shallow learning algorithms?\"\n",
      "\n",
      "3. **Applications:**\n",
      "   - \"What are the common applications of deep learning across different industries?\"\n",
      "\n",
      "4. **Technical Aspects:**\n",
      "   - \"What are the key components, architectures, and algorithms used in deep learning models?\"\n",
      "\n",
      "5. **Historical Context:**\n",
      "   - \"What is the history and evolution of deep learning technology from its inception to the present?\"\n",
      "\n",
      "6. **Learning Resources:**\n",
      "   - \"Where can I find comprehensive resources, tutorials, and courses to learn about deep learning?\"\n",
      "\n",
      "7. **Current Trends:**\n",
      "   - \"What are the latest trends, advancements, and research in deep learning as of 2023?\"\n",
      "\n",
      "8. **Practical Implementation:**\n",
      "   - \"How can deep learning be implemented in a practical project, including tools and frameworks?\"\n",
      "\n",
      "By specifying the context or particular aspect of deep learning you're interested in, you are more likely to obtain relevant and high-quality search results.\n",
      "Node 'transform_query':\n",
      "{'question': 'Certainly! To optimize the question \"What is Deep Learning?\" for web search, let\\'s consider the various aspects and contexts a searcher might be interested in. Here are several refined versions of the question that can yield more targeted and useful results:\\n\\n1. **General Definition:**\\n   - \"What is the definition of deep learning in the field of artificial intelligence?\"\\n\\n2. **Comparison with Other Techniques:**\\n   - \"How does deep learning differ from traditional machine learning and shallow learning algorithms?\"\\n\\n3. **Applications:**\\n   - \"What are the common applications of deep learning across different industries?\"\\n\\n4. **Technical Aspects:**\\n   - \"What are the key components, architectures, and algorithms used in deep learning models?\"\\n\\n5. **Historical Context:**\\n   - \"What is the history and evolution of deep learning technology from its inception to the present?\"\\n\\n6. **Learning Resources:**\\n   - \"Where can I find comprehensive resources, tutorials, and courses to learn about deep learning?\"\\n\\n7. **Current Trends:**\\n   - \"What are the latest trends, advancements, and research in deep learning as of 2023?\"\\n\\n8. **Practical Implementation:**\\n   - \"How can deep learning be implemented in a practical project, including tools and frameworks?\"\\n\\nBy specifying the context or particular aspect of deep learning you\\'re interested in, you are more likely to obtain relevant and high-quality search results.', 'documents': [], 'times_transformed': 2, 'web_search': 'Yes'}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: no\n",
      "---GRADE: DOCUMENTS NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "Node 'grade_documents':\n",
      "---TRANSFORM QUERY---\n",
      "---NEW QUESTION---\n",
      "Certainly! Here's a refined and optimized version of the question \"What is Deep Learning?\" for web search, considering various contexts and aspects a searcher might be interested in:\n",
      "\n",
      "---\n",
      "\n",
      "**Comprehensive Overview of Deep Learning:**\n",
      "\n",
      "\"What is deep learning, and how does it differ from traditional machine learning and shallow learning algorithms? Include information on its definition, key components, architectures, algorithms, historical evolution, common applications across industries, current trends as of 2023, practical implementation tips, and recommended resources for learning.\"\n",
      "\n",
      "---\n",
      "\n",
      "This consolidated question aims to capture a broad yet detailed understanding of deep learning, making it more likely to yield comprehensive and relevant search results.\n",
      "Node 'transform_query':\n",
      "{'question': 'Certainly! Here\\'s a refined and optimized version of the question \"What is Deep Learning?\" for web search, considering various contexts and aspects a searcher might be interested in:\\n\\n---\\n\\n**Comprehensive Overview of Deep Learning:**\\n\\n\"What is deep learning, and how does it differ from traditional machine learning and shallow learning algorithms? Include information on its definition, key components, architectures, algorithms, historical evolution, common applications across industries, current trends as of 2023, practical implementation tips, and recommended resources for learning.\"\\n\\n---\\n\\nThis consolidated question aims to capture a broad yet detailed understanding of deep learning, making it more likely to yield comprehensive and relevant search results.', 'documents': [], 'times_transformed': 3, 'web_search': 'Yes'}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: no\n",
      "---GRADE: DOCUMENTS NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION AND WE HAVE TRANSFORMED 3 TIMES, GENERATE---\n",
      "Node 'grade_documents':\n",
      "---GENERATE---\n",
      "Node 'generate':\n",
      " Deep learning is a subset of machine learning that uses artificial neural networks with multiple layers to learn and analyze data. It differs from traditional machine learning and shallow learning algorithms in its ability to learn complex patterns, make predictions, and extract meaningful features from data. Deep learning architectures typically consist of multiple layers, including an input layer, hidden layers, and an output layer. The layers are connected through weights and biases, allowing the network to learn and make predictions based on the input data.\n",
      "\n",
      "Deep learning has found widespread applications in various industries, including image and speech recognition, natural language processing, autonomous vehicles, and medical diagnosis. It has achieved state-of-the-art performance in tasks such as image classification, object detection, and speech recognition, surpassing human-level accuracy in some cases.\n",
      "\n",
      "Current trends in deep learning include the development of larger and deeper neural networks, the use of specialized hardware such as GPUs for faster training, and the integration of attention mechanisms and self-supervised learning to improve performance.\n",
      "\n",
      "Practical implementation tips for deep learning include selecting an appropriate algorithm, optimizing hyperparameters, and using appropriate data preprocessing techniques. Recommended resources for learning deep learning include online courses, tutorials, and conferences.\n",
      "\n",
      "I hope this comprehensive overview provides a solid foundation for your research on deep learning.\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"What is Deep learning?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        print(f\"Node '{key}':\")\n",
    "# Final generation\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceae079",
   "metadata": {},
   "source": [
    "Graph Initialization and Initial Retrieval: \n",
    "The LangGraph execution begins at the START node with the input question \"What is Deep learning?\" and immediately flows through an unconditional edge to the **set_state** node, which initializes the graph state by setting `times_transformed = `0. The execution then follows a direct edge to the **retrieve** node, which queries the Chroma vector database using Amazon Titan embeddings and successfully retrieves 4 documents from the knowledge base containing agent-related articles.\n",
    "\n",
    "Document Grading and Conditional Routing: \n",
    "The flow continues via an unconditional edge to the **grade_documents** node, where each retrieved document undergoes relevance assessment using the Amazon Titan Express LLM. *All 4 documents receive a binary score of \"no\" (irrelevant)*, causing the node to set `web_search = \"Yes` and return an empty filtered document list. This triggers the **decide_to_generate** conditional edge, which evaluates two conditions: the web_search flag status and the transformation counter. Since `web_search == \"Yes\"` and `times_transformed < 3`, the conditional logic routes execution to the \"should_transform_query\" path.\n",
    "\n",
    "Query Transformation Loop: \n",
    "The **transform_query** node executes, using the Amazon Nova Pro LLM to rewrite the original question into a more detailed, search-optimized version while `incrementing times_transformed = 1`. An unconditional edge returns execution to the **retrieve** node, creating a feedback loop. This cycle repeats three times: retrieve ‚Üí grade_documents ‚Üí decide_to_generate ‚Üí transform_query ‚Üí retrieve. During the first two iterations, all retrieved documents continue to receive \"no\" grades, but the conditional edge logic allows continued transformation since the retry limit hasn't been reached.\n",
    "\n",
    "Successful Resolution and Answer Generation:\n",
    "On the fourth retrieval attempt (after the third query transformation), the **grade_documents** node finally identifies one document as relevant (grade = \"yes\"), setting `web_search = \"No\"` and passing the filtered document forward. The **decide_to_generate** conditional edge now evaluates `web_search == \"No\"` and routes execution to the \"**should_generate**\" path, leading to the generate node. This final node uses the Titan Lite LLM with the RAG chain to create a comprehensive answer using the single relevant document as context, before flowing through an unconditional edge to the END node, successfully completing the workflow.\n",
    "\n",
    "```mermaid\n",
    "---\n",
    "config:\n",
    "  theme: neo-dark\n",
    "  look: handDrawn\n",
    "---\n",
    "graph LR\n",
    "    subgraph \"Execution Sequence - Complex Path with Query Transformations\"\n",
    "        INPUT[üîç Input Question<br/>What is Deep learning?]\n",
    "        \n",
    "        subgraph \"Cycle 1 - Initial Attempt\"\n",
    "            INIT1[üöÄ set_state<br/>times_transformed = 0]\n",
    "            RETRIEVE1[üìö retrieve<br/>Get 4 documents]\n",
    "            GRADE1[üéØ grade_documents<br/>All 4 docs: Grade = NO ‚ùå<br/>web_search = Yes]\n",
    "            DECISION1{ü§î decide_to_generate<br/>web_search = Yes<br/>times_transformed = 0 < 3}\n",
    "            TRANSFORM1[üîÑ transform_query<br/>Rewrite attempt #1<br/>times_transformed = 1]\n",
    "        end\n",
    "        \n",
    "        subgraph \"Cycle 2 - First Retry\"\n",
    "            RETRIEVE2[üìö retrieve<br/>Get 4 new documents]\n",
    "            GRADE2[üéØ grade_documents<br/>All 4 docs: Grade = NO ‚ùå<br/>web_search = Yes]\n",
    "            DECISION2{ü§î decide_to_generate<br/>web_search = Yes<br/>times_transformed = 1 < 3}\n",
    "            TRANSFORM2[üîÑ transform_query<br/>Rewrite attempt #2<br/>times_transformed = 2]\n",
    "        end\n",
    "        \n",
    "        subgraph \"Cycle 3 - Second Retry\"\n",
    "            RETRIEVE3[üìö retrieve<br/>Get 4 new documents]\n",
    "            GRADE3[üéØ grade_documents<br/>All 4 docs: Grade = NO ‚ùå<br/>web_search = Yes]\n",
    "            DECISION3{ü§î decide_to_generate<br/>web_search = Yes<br/>times_transformed = 2 < 3}\n",
    "            TRANSFORM3[üîÑ transform_query<br/>Rewrite attempt #3<br/>times_transformed = 3]\n",
    "        end\n",
    "        \n",
    "        subgraph \"Cycle 4 - Final Attempt\"\n",
    "            RETRIEVE4[üìö retrieve<br/>Get 4 documents]\n",
    "            GRADE4[üéØ grade_documents<br/>1 doc: Grade = YES ‚úÖ<br/>3 docs: Grade = NO ‚ùå<br/>web_search = No]\n",
    "            DECISION4{ü§î decide_to_generate<br/>web_search = No<br/>Has relevant document}\n",
    "            GENERATE[üí° generate<br/>Create comprehensive answer<br/>Using 1 relevant document]\n",
    "        end\n",
    "        \n",
    "        ANSWER[üìù Final Answer<br/>Deep learning vs traditional ML<br/>explanation with industry examples]\n",
    "    end\n",
    "    \n",
    "    %% Flow connections\n",
    "    INPUT --> INIT1\n",
    "    INIT1 --> RETRIEVE1\n",
    "    RETRIEVE1 --> GRADE1\n",
    "    GRADE1 --> DECISION1\n",
    "    \n",
    "    %% Cycle 1\n",
    "    DECISION1 -->|should_transform_query| TRANSFORM1\n",
    "    TRANSFORM1 --> RETRIEVE2\n",
    "    \n",
    "    %% Cycle 2  \n",
    "    RETRIEVE2 --> GRADE2\n",
    "    GRADE2 --> DECISION2\n",
    "    DECISION2 -->|should_transform_query| TRANSFORM2\n",
    "    TRANSFORM2 --> RETRIEVE3\n",
    "    \n",
    "    %% Cycle 3\n",
    "    RETRIEVE3 --> GRADE3\n",
    "    GRADE3 --> DECISION3\n",
    "    DECISION3 -->|should_transform_query| TRANSFORM3\n",
    "    TRANSFORM3 --> RETRIEVE4\n",
    "    \n",
    "    %% Cycle 4 - Success\n",
    "    RETRIEVE4 --> GRADE4\n",
    "    GRADE4 --> DECISION4\n",
    "    DECISION4 -->|should_generate| GENERATE\n",
    "    GENERATE --> ANSWER\n",
    "    \n",
    "    %% Styling\n",
    "    classDef executed fill:#c8e6c9,stroke:#4caf50,stroke-width:2px\n",
    "    classDef decision fill:#fff3e0,stroke:#ff9800,stroke-width:2px\n",
    "    classDef transform fill:#e1f5fe,stroke:#2196f3,stroke-width:2px\n",
    "    classDef inputOutput fill:#f3e5f5,stroke:#9c27b0,stroke-width:3px\n",
    "    classDef cycle1 fill:#fff3e0,stroke:#ffc107,stroke-width:1px\n",
    "    classDef cycle2 fill:#f0f4ff,stroke:#3f51b5,stroke-width:1px\n",
    "    classDef cycle3 fill:#f1f8e9,stroke:#8bc34a,stroke-width:1px\n",
    "    classDef cycle4 fill:#e8f5e8,stroke:#4caf50,stroke-width:1px\n",
    "    \n",
    "    class INPUT,ANSWER inputOutput\n",
    "    class DECISION1,DECISION2,DECISION3,DECISION4 decision\n",
    "    class TRANSFORM1,TRANSFORM2,TRANSFORM3 transform\n",
    "    class INIT1,RETRIEVE1,GRADE1,RETRIEVE2,GRADE2,RETRIEVE3,GRADE3,RETRIEVE4,GRADE4,GENERATE executed\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ae5140",
   "metadata": {},
   "source": [
    "In the above question, no document returned. But in the following question documents are retured.\n",
    "\n",
    "#### Senario: 3 documents returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e93f5f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---SET STATE---\n",
      "Node 'set_state':\n",
      "{'question': 'What is Short term memory?', 'times_transformed': 0}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: yes\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: yes\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: yes\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: no\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "Node 'grade_documents':\n",
      "---GENERATE---\n",
      "Node 'generate':\n",
      " Short-term memory is believed to have the capacity of about 7 items and lasts for 20-30 seconds.\n"
     ]
    }
   ],
   "source": [
    "# return 3 documents\n",
    "inputs = {\"question\": \"What is Short term memory?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        print(f\"Node '{key}':\")\n",
    "# Final generation\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817d2ed5",
   "metadata": {},
   "source": [
    "Step 1: Graph Initialization and Entry Point:\n",
    "\n",
    "Node: `START` ‚Üí **set_state**\n",
    "\n",
    "Edge: Unconditional edge from `START` to **set_state**\n",
    "Execution: The LangGraph workflow begins execution with the input `{\"question\": \"What is Short term memory?\"}`. The framework automatically routes to the **set_state** node via the predefined `workflow.add_edge(START, \"set_state\")` configuration. This node initializes the critical state variable `times_transformed = 0`, which serves as a retry counter for query transformation attempts. The state is now `{'question': 'What is Short term memory?', 'times_transformed': 0}`, establishing the foundation for all subsequent node operations.\n",
    "\n",
    "\n",
    "Step 2: Document Retrieval from Vector Database\n",
    "\n",
    "Node: **set_state** ‚Üí **retrieve**\n",
    "\n",
    "Edge: Unconditional edge defined by `workflow.add_edge(\"set_state\", \"retrieve\")`\n",
    "\n",
    "Execution: The workflow transitions to the `retrieve` node, which extracts the question from the shared state and invokes `retriever.get_relevant_documents(question)`. This operation queries the Chroma vector database using Amazon Titan embeddings to perform semantic similarity search against the pre-indexed agent-related articles. The retrieval successfully returns 4 document chunks, all sourced from \"https://lilianweng.github.io/posts/2023-06-23-agent/\", which are added to the state as `{\"documents\": [Document1, Document2, Document3, Document4]}`.\n",
    "\n",
    "Step 3: Document Relevance Assessment\n",
    "\n",
    "Node: **retrieve** ‚Üí **grade_documents**\n",
    "\n",
    "Edge: Unconditional edge defined by `workflow.add_edge(\"retrieve\", \"grade_documents\")`\n",
    "\n",
    "Execution: The `grade_documents` node processes each retrieved document through the `retrieval_grader()` function, which uses the Amazon Titan Express LLM to evaluate relevance. For each document, it creates a prompt containing both the question and document content, then receives a binary classification (\"yes\" or \"no\"). *The grading results show: Document 1-3 receive \"Grade: yes\" (relevant), while Document 4 receives \"Grade: no\" (irrelevant)*. Since 3 documents are deemed relevant, the node sets `web_search = \"No\"` and updates the state with `{\"documents\": [Document1, Document2, Document3], \"web_search\": \"No\"}`.\n",
    "\n",
    "Step 4: Conditional Decision Logic\n",
    "\n",
    "Node: **grade_documents** ‚Üí **decide_to_generate** (conditional edge)\n",
    "\n",
    "Edge: Conditional edge defined by `workflow.add_conditional_edges()` with routing logic\n",
    "\n",
    "Conditions Evaluated: The `decide_to_generate()` function examines the `web_search` flag in the current state. Since `web_search == \"No\"` (indicating relevant documents were found), the condition `if web_search == \"Yes\"` evaluates to False. The function bypasses the retry logic checks (`times_transformed >= 3`) and proceeds directly to the else clause, printing \"---DECISION: GENERATE---\" and returning the string \"**should_generate**\". This return value determines the next node via the conditional routing dictionary `{\"should_transform_query\": \"transform_query\", \"should_generate\": \"generate\"}`.\n",
    "\n",
    "Step 5: Answer Generation with RAG Chain\n",
    "\n",
    "Node: **grade_documents** ‚Üí **generate**\n",
    "\n",
    "Edge: Conditional edge routing to `\"should_generate\": \"generate\"`\n",
    "\n",
    "Execution: The **generate** node receives the filtered relevant documents and the original question from the shared state. It invokes the `rag_chain.invoke()` method, which consists of a sequential pipeline: prompt formatting ‚Üí Amazon Titan Lite LLM processing ‚Üí string output parsing. The RAG chain combines the 3 relevant documents using `format_docs()` function and the hub-pulled \"rlm/rag-prompt\" template to create a contextual prompt. The Titan Lite model generates the response: \"Short-term memory is believed to have the capacity of about 7 items and lasts for 20-30 seconds.\" This generation is added to the state as `{\"generation\": \"...\"}`.\n",
    "\n",
    "Step 6: Workflow Termination\n",
    "\n",
    "Node: **generate** ‚Üí END\n",
    "\n",
    "Edge: Unconditional edge defined by `workflow.add_edge(\"generate\", END)`\n",
    "\n",
    "Execution: The **generate** node completes successfully and the workflow follows the unconditional edge to the END node, terminating the graph execution. The final state contains all accumulated data: the original question, the 3 relevant documents, the generated answer, and metadata including `times_transformed = 0` and `web_search = \"No\"`. The `app.stream()` method yields the final output, allowing the calling code to extract and display the generated answer.\n",
    "\n",
    "Path Analysis: Direct Success Route\n",
    "\n",
    "Path Taken: START ‚Üí set_state ‚Üí retrieve ‚Üí grade_documents ‚Üí decide_to_generate ‚Üí generate ‚Üí END\n",
    "\n",
    "Path Not Taken: The **transform_query** node remains unexecuted because the conditional logic in `decide_to_generate()` determined that sufficient relevant documents were available. The feedback loop edge **workflow.add_edge(\"transform_query\", \"retrieve\")** was never traversed, demonstrating the efficiency of the LangGraph's adaptive routing when the initial query successfully matches the knowledge base content. This execution represents the optimal scenario where no query refinement or multiple retrieval attempts are necessary.\n",
    "\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    INPUT[üîç Input Question<br/>What is Short term memory?]\n",
    "    \n",
    "    INIT[üöÄ set_state<br/>times_transformed = 0]\n",
    "    \n",
    "    RETRIEVE[üìö retrieve<br/>Get 4 documents from vector DB]\n",
    "    \n",
    "    GRADE[üéØ grade_documents<br/>3 docs: relevant ‚úÖ<br/>1 doc: not relevant ‚ùå]\n",
    "    \n",
    "    DECISION{ü§î decide_to_generate<br/>Check: Has relevant docs?}\n",
    "    \n",
    "    TRANSFORM[üîÑ transform_query<br/>‚ùå NOT EXECUTED<br/>Query rewriting]\n",
    "    \n",
    "    GENERATE[üí° generate<br/>Create answer using<br/>3 relevant documents]\n",
    "    \n",
    "    ANSWER[üìù Final Answer<br/>Short-term memory capacity:<br/>~7 items, lasts 20-30 seconds]\n",
    "    \n",
    "    %% Main execution flow (Cell 29 actual path)\n",
    "    INPUT --> INIT\n",
    "    INIT --> RETRIEVE\n",
    "    RETRIEVE --> GRADE\n",
    "    GRADE --> DECISION\n",
    "    \n",
    "    %% Decision branches\n",
    "    DECISION -->|‚úÖ YES: Has relevant docs<br/>Route to generate| GENERATE\n",
    "    DECISION -.->|‚ùå NO: Transform query<br/>NOT TAKEN in Cell 29| TRANSFORM\n",
    "    \n",
    "    %% Transform loop (not executed in this case)\n",
    "    TRANSFORM -.-> RETRIEVE\n",
    "    \n",
    "    %% Final result\n",
    "    GENERATE --> ANSWER\n",
    "    \n",
    "    %% Styling\n",
    "    classDef executed fill:#c8e6c9,stroke:#4caf50,stroke-width:3px\n",
    "    classDef notExecuted fill:#ffcdd2,stroke:#f44336,stroke-width:2px,stroke-dasharray: 5 5\n",
    "    classDef decision fill:#fff3e0,stroke:#ff9800,stroke-width:3px\n",
    "    classDef inputOutput fill:#e3f2fd,stroke:#2196f3,stroke-width:3px\n",
    "    \n",
    "    class INPUT,ANSWER inputOutput\n",
    "    class INIT,RETRIEVE,GRADE,GENERATE executed\n",
    "    class TRANSFORM notExecuted\n",
    "    class DECISION decision\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85dae87",
   "metadata": {},
   "source": [
    "[^1]: [What is an AI agent?](https://blog.langchain.com/what-is-an-agent/){:target=\"_blank\"}\n",
    "\n",
    "[^2]: [Oreilly AI Agents](https://github.com/sinanuozdemir/oreilly-ai-agents/blob/main/notebooks/LangGraph_Hello_World.ipynb){:target=\"_blank\"}\n",
    "\n",
    "[^3]: [LangGraph](https://www.langchain.com/langgraph){:target=\"_blank\"}\n",
    "\n",
    "[^4]: [Stanford STORM Research Project](https://storm-project.stanford.edu/research/storm/){:target=\"_blank\"}\n",
    "\n",
    "[^5]: [How to return structured data from a model \\| ü¶úÔ∏èüîó LangChain](https://python.langchain.com/docs/how_to/structured_output/)\n",
    "\n",
    "[^6]: [LangChain Expression Language (LCEL) \\| ü¶úÔ∏èüîó LangChain](https://python.langchain.com/docs/concepts/lcel/)\n",
    "\n",
    "[^7]: [langchain-community](https://python.langchain.com/api_reference/community/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-bedrock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
