{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d1591dd",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  LangGraph Agent for AWS BedRock\n",
    "date:   2025-08-29\n",
    "categories: [AI, AWS]\n",
    "mermaid: true\n",
    "typora-root-url: /Users/ojitha/GitHub/ojitha.github.io\n",
    "typora-copy-images-to: ../assets/images/${filename}\n",
    "---\n",
    "\n",
    "<style>\n",
    "/* Styles for the two-column layout */\n",
    ".image-text-container {\n",
    "    display: flex; /* Enables flexbox */\n",
    "    flex-wrap: wrap; /* Allows columns to stack on small screens */\n",
    "    gap: 20px; /* Space between the image and text */\n",
    "    align-items: center; /* Vertically centers content in columns */\n",
    "    margin-bottom: 20px; /* Space below this section */\n",
    "}\n",
    "\n",
    ".image-column {\n",
    "    flex: 1; /* Allows this column to grow */\n",
    "    min-width: 250px; /* Minimum width for the image column before stacking */\n",
    "    max-width: 40%; /* Maximum width for the image column to not take up too much space initially */\n",
    "    box-sizing: border-box; /* Include padding/border in element's total width/height */\n",
    "}\n",
    "\n",
    ".text-column {\n",
    "    flex: 2; /* Allows this column to grow more (e.g., twice as much as image-column) */\n",
    "    min-width: 300px; /* Minimum width for the text column before stacking */\n",
    "    box-sizing: border-box;\n",
    "}\n",
    "\n",
    "</style>\n",
    "\n",
    "<div class=\"image-text-container\">\n",
    "    <div class=\"image-column\">\n",
    "        <img src=\"/assets/images/2025-07-25-Scala-basics/scala_basics.png\" alt=\"Scala basics\" width=\"150\" height=\"150\">\n",
    "    </div>\n",
    "    <div class=\"text-column\">\n",
    "<p>Demonstrates the creation of an intelligent RAG (Retrieval Augmented Generation) agent using LangGraph with AWS Bedrock. The implementation showcases advanced AI agent patterns including document relevance grading, query transformation, and conditional workflow execution.</p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<!--more-->\n",
    "\n",
    "------\n",
    "\n",
    "* TOC\n",
    "{:toc}\n",
    "------\n",
    "\n",
    "## Introduction\n",
    "According to the LangChain[^1], AI agent is a system that uses an LLM to decide the control flow of an application: but there is no exact difition. Instead LangChain introduce the different levels of autonomy:\n",
    "\n",
    "![Autonomy of AI Agents](https://blog.langchain.com/content/images/2024/06/Screenshot-2024-06-28-at-7.33.10-PM.png){:width=\"50%\" height=\"50%\"}\n",
    "\n",
    "\n",
    "> LangGraph, the agent orchestrator to help with building, running, and interacting with agents, and LangSmith, the testing and observability platform for LLM apps.\n",
    "{:.green}\n",
    "\n",
    "In the above spectrum, typically LangGraph can be used to develop Agents for the levels 4,5 and 6 where:\n",
    "\n",
    "- Router: LLM routes inputs into specific downstream workflows.\n",
    "- State Machine: LLMs determine whether to continue base on the state.\n",
    "- Autonomous: System build tools, remembers them and uses them in future steps.\n",
    "\n",
    "> Router level is high application reliability but at autonomus level the lowest application reliability.\n",
    "{:.yellow}\n",
    "\n",
    "Three core principles in the LangGraph:\n",
    "\n",
    "1. **Controllability**\n",
    "    1. *Precise Control Over Systems* because LangGraph is a low-level framework that gives you fine-grained control over your Al systems.\n",
    "    2. *Why Control Matters* which \n",
    "        1. Enhances the accuracy and guidability of Al agent workflows.\n",
    "        2. Allows customization to meet specific requirements/constraints.\n",
    "        3. Possible to define exact behavior for each step, ensuring predictable outcomes.\n",
    "2. **Human in Loop**: Enables humans to make informed decisions at crucial steps in the workflow.\n",
    "3. **Streaming**: LangGraph supports streaming of events and tokens, providing immediate feedback to end-users.\n",
    "\n",
    "In this article, I am plannig to create the following AI Agent architecture using LangGrap:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    subgraph \"Input Layer\"\n",
    "        INPUT[User Question<br/>e.g. What is Deep learning?]\n",
    "    end\n",
    "    \n",
    "    subgraph \"LangGraph Workflow\"\n",
    "        INIT[set_state<br/>Initialize State<br/>times_transformed = 0]\n",
    "        \n",
    "        RETRIEVE[retrieve<br/>Vector Search<br/>Chroma DB + Titan Embeddings]\n",
    "        \n",
    "        GRADE[grade_documents<br/>Document Relevance Grading<br/>Titan Express LLM]\n",
    "        \n",
    "        DECISION{decide_to_generate<br/>Conditional Logic<br/>Check relevance & retry count}\n",
    "        \n",
    "        TRANSFORM[transform_query<br/>Query Rewriting<br/>Nova Pro LLM]\n",
    "        \n",
    "        GENERATE[generate<br/>Answer Generation<br/>Titan Lite LLM + RAG Chain]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Data Sources\"\n",
    "        VECTOR[(Vector Store<br/>Chroma DB<br/>Agent-related Articles)]\n",
    "        EMBEDDINGS[Amazon Titan<br/>Embedding Model<br/>Text to Vectors]\n",
    "    end\n",
    "    \n",
    "    subgraph \"LLM Models\"\n",
    "        TITAN_EXPRESS[Amazon Titan Express<br/>Document Grading]\n",
    "        NOVA_PRO[Amazon Nova Pro<br/>Query Rewriting]\n",
    "        TITAN_LITE[Amazon Titan Lite<br/>Answer Generation]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Output Layer\"\n",
    "        ANSWER[Generated Answer<br/>Contextual Response]\n",
    "        FAILURE[Fallback Answer<br/>I don't know response]\n",
    "    end\n",
    "    \n",
    "    INPUT --> INIT\n",
    "    INIT --> RETRIEVE\n",
    "    RETRIEVE --> GRADE\n",
    "    GRADE --> DECISION\n",
    "    \n",
    "    DECISION -->|No relevant docs<br/>times_transformed < 3| TRANSFORM\n",
    "    DECISION -->|Has relevant docs<br/>OR retry limit reached| GENERATE\n",
    "    \n",
    "    TRANSFORM --> RETRIEVE\n",
    "    GENERATE --> ANSWER\n",
    "    GENERATE --> FAILURE\n",
    "    \n",
    "    RETRIEVE -.-> VECTOR\n",
    "    VECTOR -.-> EMBEDDINGS\n",
    "    GRADE -.-> TITAN_EXPRESS\n",
    "    TRANSFORM -.-> NOVA_PRO\n",
    "    GENERATE -.-> TITAN_LITE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ca5c51",
   "metadata": {},
   "source": [
    "AWS CLI command to list Bedrock models\n",
    "\n",
    "- Queries AWS Bedrock for Amazon provider models\n",
    "- Filters for text output modality and provisioned inference\n",
    "- Uses jq for JSON parsing to extract model IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5eb69fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon.titan-text-lite-v1:0:4k\n",
      "amazon.titan-text-express-v1:0:8k\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "aws bedrock list-foundation-models \\\n",
    "    --by-provider \"Amazon\" \\\n",
    "    --by-output-modality TEXT \\\n",
    "    --by-inference-type PROVISIONED \\\n",
    "    --output json | jq -r '.modelSummaries[] | .modelId'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99065aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Intelligently splits documents while preserving context\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Loads documents from web URLs\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Vector database for similarity search\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# AWS Bedrock embedding model integration\n",
    "from langchain_aws import BedrockEmbeddings \n",
    "\n",
    "\n",
    "boto3.set_stream_logger('', logging.ERROR)\n",
    "\n",
    "# Initialize AWS Bedrock client\n",
    "bedrock_client = boto3.client(\n",
    "    service_name='bedrock-runtime'\n",
    "    ,region_name='ap-southeast-2'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f56dca",
   "metadata": {},
   "source": [
    "Create Bedrock embeddings instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e39f4ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = BedrockEmbeddings(\n",
    "    client=bedrock_client,\n",
    "    model_id=\"amazon.titan-embed-text-v1\"  # or use v2 for better performance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156406d",
   "metadata": {},
   "source": [
    "## Embeddings pipeline\n",
    "*Embeddings* are numerical representations of text, where similar pieces of text are converted into similar vectors (arrays of numbers). Think of them as coordinates in a multi-dimensional space where semantically related content clusters together.\n",
    "\n",
    "How Embeddings Capture Meaning\n",
    "\n",
    "- *Semantic similarity*: Words/phrases with similar meanings have similar vectors\n",
    "- *Context awareness*: \"bank\" (river) vs \"bank\" (financial) get different embeddings\n",
    "- *Dimensional encoding*: Each dimension captures different aspects of meaning\n",
    "\n",
    "What is need to do:\n",
    "\n",
    "1. Load documents from URLs\n",
    "2. Split into chunks (250 tokens each)\n",
    "3. Each chunk becomes a separate document\n",
    "4. Amazon Titan reads each text chunk\n",
    "5. Converts text into a 1536-dimensional vector\n",
    "6. Each dimension represents learned semantic features\n",
    "\n",
    "Here the complete RAG data preparation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ddab9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tiktoken-based text splitter\n",
      "Created 187 document chunks\n"
     ]
    }
   ],
   "source": [
    "# Initialize Bedrock embeddings with Amazon Titan model\n",
    "embeddings = BedrockEmbeddings(\n",
    "    client=bedrock_client,\n",
    "    model_id=\"amazon.titan-embed-image-v1\"  # Using Amazon Titan embedding model\n",
    ")\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "\n",
    "# Load documents from URLs\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Option 1: Use tiktoken-based splitter (if tiktoken is installed)\n",
    "try:\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=250, chunk_overlap=0\n",
    "    )\n",
    "    print(\"Using tiktoken-based text splitter\")\n",
    "except ImportError:\n",
    "    # Option 2: Fallback to character-based splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=250,\n",
    "        chunk_overlap=0,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    print(\"Using character-based text splitter (tiktoken not available)\")\n",
    "\n",
    "# Split documents into chunks\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "print(f\"Created {len(doc_splits)} document chunks\")\n",
    "\n",
    "# Add to vectorDB with Bedrock embeddings\n",
    "vectorstore = Chroma.from_documents( # Creates vector store from document chunks\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma-bedrock\",  # Changed collection name for clarity\n",
    "    embedding=embeddings,  # Using Bedrock embeddings instead of OpenAI\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84201305",
   "metadata": {},
   "source": [
    "In the above code `RecursiveCharacterTextSplitter.from_tiktoken_encoder()` is the *Token-aware* text splitting for LLM context limits. \n",
    "It used the following Parameters:\n",
    "\n",
    "- chunk_size=250: Maximum tokens per chunk\n",
    "- chunk_overlap=0: No overlap between chunks\n",
    "\n",
    "Creates vector store from document chunks: That will automatically embeds all documents and stores in vector DB. The last statement of the above code creates retriever interface for similarity search which returns `top-k` similar documents for queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061c3823",
   "metadata": {},
   "source": [
    "Test document retrieval functionality. Test the vector database contents. It should return `List[Document]` with similarity-ranked results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51e33491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short-Term Memory (STM) or Working Memory: It stor Ellipsis https://lilianweng.github.io/posts/2023-06-23-agent/\n",
      "Short-term memory: I would consider all the in-con Ellipsis https://lilianweng.github.io/posts/2023-06-23-agent/\n",
      "Sensory memory as learning embedding representatio Ellipsis https://lilianweng.github.io/posts/2023-06-23-agent/\n",
      "Constraints:\n",
      "1. ~4000 word limit for short term me Ellipsis https://lilianweng.github.io/posts/2023-06-23-agent/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8w/ytv2_9yd0nld4gbqjwbs4yw40000gn/T/ipykernel_9547/3939294398.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents (question)\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Short-Term Memory?\"\n",
    "docs = retriever.get_relevant_documents (question)\n",
    "for doc in docs:\n",
    "    print (doc.page_content[:50], ... , doc.metadata['source'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e50f60",
   "metadata": {},
   "source": [
    "## Retrieval Grader Implementation\n",
    "Grader is a document relevance assessment using structured outputs. For that this code using `ChatPromptTemplate` and `ChatBedrock`. In this case `BaseModel` is from *Pydantic*.\n",
    "\n",
    "> The function `structured_llm_grader` is the Custom Structured Output Handler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5607e4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ojitha/workspace/learn-bedrock/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3699: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "### Simple Direct Conversion: Retrieval Grader with AWS Bedrock Titan\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "import re\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "\n",
    "# Data model (unchanged)\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    client=bedrock_client,\n",
    "    model_id=\"amazon.titan-text-express-v1\",  \n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,  # Same temperature as original\n",
    "        \"maxTokenCount\": 1000\n",
    "    }\n",
    ")\n",
    "\n",
    "# Custom structured output handler (since Titan doesn't support with_structured_output directly)\n",
    "def structured_llm_grader(messages):\n",
    "    \"\"\"Custom function to handle structured output from Titan model\"\"\"\n",
    "    response = llm.invoke(messages)\n",
    "    response_text = response.content\n",
    "    \n",
    "    # Try to parse JSON response\n",
    "    try:\n",
    "        # Look for JSON pattern\n",
    "        json_match = re.search(r'\\{[^}]*\"binary_score\"[^}]*\\}', response_text, re.IGNORECASE)\n",
    "        if json_match:\n",
    "            json_data = json.loads(json_match.group())\n",
    "            return GradeDocuments(binary_score=json_data.get('binary_score', 'no'))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Fallback: simple keyword detection\n",
    "    if 'yes' in response_text.lower() and 'no' not in response_text.lower():\n",
    "        return GradeDocuments(binary_score='yes')\n",
    "    else:\n",
    "        return GradeDocuments(binary_score='no')\n",
    "\n",
    "# Enhanced prompt with JSON format requirement\n",
    "system = \"\"\"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "])\n",
    "\n",
    "# Create the retrieval grader function\n",
    "def retrieval_grader(inputs):\n",
    "    \"\"\"Grade document relevance using Bedrock Titan model\"\"\"\n",
    "    messages = grade_prompt.format_prompt(**inputs).to_messages()\n",
    "    return structured_llm_grader(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35412e73",
   "metadata": {},
   "source": [
    "> It is important to notice that `retrieval_grader(inputs)` is the grader function.\n",
    "If you have a retriever from the embeddings code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5145b69",
   "metadata": {},
   "source": [
    "Testing retriver:\n",
    "\n",
    "1. Retrieve documents for question\n",
    "2. Grade each document for relevance\n",
    "3. Return structured GradeDocuments object\n",
    "4. Print binary scores (\"yes\"/\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd54f43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n",
      "binary_score='yes'\n",
      "binary_score='no'\n",
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "# Test example (same as original structure)\n",
    "question = \"Short-Term Memory\"\n",
    "\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "for doc in docs:\n",
    "    doc_txt = doc.page_content\n",
    "    result = retrieval_grader({\"question\": question, \"document\": doc_txt})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46ba8e3",
   "metadata": {},
   "source": [
    "Build generation pipeline using LangChain Hub which will download the prebuild prompts. For example, **rlm/rag-prompt** is Standard RAG question-answering template. \n",
    "\n",
    "The hub prompt contains:\n",
    "\n",
    "1. Role definition: \"assistant for question-answering tasks\"\n",
    "2. Context injection: Uses retrieved documents\n",
    "3. Constraints: \"three sentences maximum\", \"say you don't know if uncertain\"\n",
    "4. Template variables: `{question}` and `{context}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af28122b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ojitha/workspace/learn-bedrock/.venv/lib/python3.13/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.chat.HumanMessagePromptTemplate'>\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: {question} \n",
      "Context: {context} \n",
      "Answer:\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "for message in prompt.messages:\n",
    "    print(type(message))\n",
    "    print(message.prompt.template)\n",
    "    print('-----')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0395d1",
   "metadata": {},
   "source": [
    "> The `StrOutputParser` is optiona, but better extracts string content from LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef9e6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Short-term memory is believed to have the capacity of about 7 items and lasts for 20-30 seconds.\n"
     ]
    }
   ],
   "source": [
    "# This time LLM is Amazon Titan Lite\n",
    "llm_another = ChatBedrock(\n",
    "    client=bedrock_client,\n",
    "    model_id=\"amazon.titan-text-lite-v1\",  \n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,  # Same temperature as original\n",
    "        \"maxTokenCount\": 1000\n",
    "    }\n",
    ")\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm_another | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": format_docs(docs), \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ced183",
   "metadata": {},
   "source": [
    "### LangChain Chain Pattern\n",
    "\n",
    "Using the `|` RAG chain can be created where \n",
    "\n",
    "- Sequential processing pipeline (Prompt Template formats input with context and question)\n",
    "- Each component feeds into the next (Titan Lite LLM generates answer from formatted prompt)\n",
    "- Enables functional composition of LLM operations (Output Parser extracts clean text response)\n",
    "\n",
    "### Query optimization using advanced LLM\n",
    "Section for query rewriting with more capable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6fe414b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A different LLM, just to show we can use multiple LLMs in our calls\n",
    "bigger_llm = ChatBedrock(\n",
    "    client=bedrock_client,\n",
    "    model_id=\"amazon.nova-pro-v1:0\",  \n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,  # Same temperature as original\n",
    "        \"maxTokenCount\": 1000\n",
    "    }\n",
    ")\n",
    "# Prompt\n",
    "system = \"\"\"You are a question re-writer that converts an input question to a better version that is optimized \\n\n",
    "     for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | bigger_llm | StrOutputParser()\n",
    "# question, question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d9b13a",
   "metadata": {},
   "source": [
    "## Build the Graph\n",
    "\n",
    "Define shared state structure for LangGraph.\n",
    "\n",
    "### State Management \n",
    "\n",
    "State management Pattern is as follows:\n",
    "\n",
    "- Shared state persists across all graph nodes\n",
    "- Each node can read and update state\n",
    "- Type hints ensure consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d21cc693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "        times_transformed: number of times the question has been re-written\n",
    "        web_search: if we should be doing a web search (not implemented in this notebook)\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]\n",
    "    times_transformed: int\n",
    "    web_search: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23437df",
   "metadata": {},
   "source": [
    "### Nodes\n",
    "Define LangGraph workflow nodes\n",
    "\n",
    "- `set_state(state)`\n",
    "    - **Purpose:** Initialize graph state\n",
    "    - **Returns:** `{\"times_transformed\": 0}`\n",
    "    - **Pattern:** Setup/initialization node\n",
    "\n",
    "- `retrieve(state)`  \n",
    "    - **Purpose:** Document retrieval from vector store\n",
    "    - **LangChain API:** `retriever.get_relevant_documents(question)`\n",
    "    - **Returns:** `{\"documents\": documents}`\n",
    "    - **State Dependency:** Reads `question` from state\n",
    "\n",
    "- `generate(state)`\n",
    "    - **Purpose:** Final answer generation\n",
    "    - **LangChain API:** `rag_chain.invoke()`  \n",
    "    - **Dependencies:** Uses `question` and `documents` from state\n",
    "    - **Returns:** `{\"generation\": generation}`\n",
    "\n",
    "- `transform_query(state)`\n",
    "    - **Purpose:** Query optimization/rewriting\n",
    "    - **LangChain API:** `question_rewriter.invoke()`\n",
    "    - **State Updates:** Increments `times_transformed`, updates `question`\n",
    "    - **Retry Logic:** Tracks transformation attempts\n",
    "\n",
    "- `grade_documents(state)`\n",
    "    - **Purpose:** Document relevance filtering\n",
    "    - **LangChain API:** `retrieval_grader()` for each document\n",
    "    - **Logic:** \n",
    "        - Filters relevant documents\n",
    "        - Sets `web_search=\"Yes\"` if no relevant docs\n",
    "        - **Returns:** `{\"documents\": filtered_docs, \"web_search\": web_search}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "059081b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "def set_state(state):\n",
    "    \"\"\"\n",
    "    Sets initial state\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---SET STATE---\")\n",
    "\n",
    "    return {\"times_transformed\": 0}\n",
    "\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    print(state)\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": format_docs(documents), \"question\": question})\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    times_transformed = state[\"times_transformed\"]\n",
    "    times_transformed += 1\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    print('---NEW QUESTION---')\n",
    "    print(better_question)\n",
    "    return {\"question\": better_question, \"times_transformed\": times_transformed}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        print(d.metadata['source'], f'Grade: {grade}')\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "    if len(filtered_docs) == 0:\n",
    "        print(\"---GRADE: DOCUMENTS NOT RELEVANT---\")\n",
    "        web_search = \"Yes\"\n",
    "    return {\"documents\": filtered_docs, \"web_search\": web_search}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f6eb5",
   "metadata": {},
   "source": [
    "### LangGraph Edge Pattern\n",
    "\n",
    "**Decision Logic:**\n",
    "1. **If `web_search == \"Yes\"`:**\n",
    "   - Check retry limit (`times_transformed >= 3`)\n",
    "   - Route to \"should_generate\" (give up) or \"should_transform_query\" (retry)\n",
    "2. **If `web_search == \"No\"`:**\n",
    "   - Route to \"should_generate\" (proceed with relevant docs)\n",
    "\n",
    "**Retry Pattern:**\n",
    "- Prevents infinite loops with transformation limit\n",
    "- Graceful degradation after 3 attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f87bb96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    web_search = state[\"web_search\"]\n",
    "    # state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # check times_transformed\n",
    "        if state[\"times_transformed\"] >= 3:\n",
    "            print(\n",
    "                \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION AND WE HAVE TRANSFORMED 3 TIMES, GENERATE---\"\n",
    "            )\n",
    "            return \"should_generate\"\n",
    "\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"should_transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"should_generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5bc0ff",
   "metadata": {},
   "source": [
    "Creates executable graph application which is a runnable graph with `.stream()` and `.invoke()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057cd5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"set_state\", set_state)  # set_state\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"set_state\")\n",
    "workflow.add_edge(\"set_state\", \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"should_transform_query\": \"transform_query\",\n",
    "        \"should_generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4406de9d",
   "metadata": {},
   "source": [
    "Here the above compiled graph:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    START([START]) --> set_state[set_state<br/>Initialize counters]\n",
    "    \n",
    "    set_state --> retrieve[retrieve<br/>Get documents from vector store]\n",
    "    \n",
    "    retrieve --> grade_documents[grade_documents<br/>Filter relevant documents]\n",
    "    \n",
    "    grade_documents --> decision{decide_to_generate<br/>Check web_search flag}\n",
    "    \n",
    "    decision -->|should_transform_query<br/>No relevant docs| transform_query[transform_query<br/>Rewrite question]\n",
    "    \n",
    "    decision -->|should_generate<br/>Has relevant docs| generate[generate<br/>Create final answer]\n",
    "    \n",
    "    transform_query --> retrieve\n",
    "    \n",
    "    generate --> END([END])\n",
    "    \n",
    "    %% Styling\n",
    "    classDef startEnd fill:#e1f5fe\n",
    "    classDef node fill:#f3e5f5\n",
    "    classDef decision fill:#fff3e0\n",
    "    classDef executed fill:#c8e6c9\n",
    "    classDef notExecuted fill:#ffcdd2\n",
    "    \n",
    "    class START,END startEnd\n",
    "    class set_state,retrieve,grade_documents,generate executed\n",
    "    class transform_query notExecuted\n",
    "    class decision decision\n",
    "```\n",
    "\n",
    "### Run the graph\n",
    "Runs the complete LangGraph workflow, stream execution with intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "32a28677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---SET STATE---\n",
      "Node 'set_state':\n",
      "{'question': 'What is Deep learning?', 'times_transformed': 0}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: no\n",
      "---GRADE: DOCUMENTS NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "Node 'grade_documents':\n",
      "---TRANSFORM QUERY---\n",
      "---NEW QUESTION---\n",
      "Certainly! To optimize the question for a web search, it's important to make it specific and clear. Here's an improved version of the question:\n",
      "\n",
      "\"What are the fundamental concepts, applications, and differences between deep learning and traditional machine learning algorithms?\"\n",
      "\n",
      "This revised question is more detailed and will likely yield more comprehensive and relevant search results. It asks not only for a definition but also for an explanation of the core concepts, real-world applications, and how deep learning differs from other machine learning approaches.\n",
      "Node 'transform_query':\n",
      "{'question': 'Certainly! To optimize the question for a web search, it\\'s important to make it specific and clear. Here\\'s an improved version of the question:\\n\\n\"What are the fundamental concepts, applications, and differences between deep learning and traditional machine learning algorithms?\"\\n\\nThis revised question is more detailed and will likely yield more comprehensive and relevant search results. It asks not only for a definition but also for an explanation of the core concepts, real-world applications, and how deep learning differs from other machine learning approaches.', 'documents': [], 'times_transformed': 1, 'web_search': 'Yes'}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "---GRADE: DOCUMENTS NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "Node 'grade_documents':\n",
      "---TRANSFORM QUERY---\n",
      "---NEW QUESTION---\n",
      "Certainly! To further optimize the question for a web search, it's essential to break it down into specific components that address the key aspects one might be looking for. Here's an even more refined version of the question:\n",
      "\n",
      "\"What are the core principles, practical applications, and key distinctions between deep learning and traditional machine learning algorithms, including examples of each approach in various industries?\"\n",
      "\n",
      "This version is designed to elicit detailed information on the foundational theories behind both deep learning and traditional machine learning, showcase how each is applied in real-world scenarios across different sectors, and highlight the critical differences between the two methodologies.\n",
      "Node 'transform_query':\n",
      "{'question': 'Certainly! To further optimize the question for a web search, it\\'s essential to break it down into specific components that address the key aspects one might be looking for. Here\\'s an even more refined version of the question:\\n\\n\"What are the core principles, practical applications, and key distinctions between deep learning and traditional machine learning algorithms, including examples of each approach in various industries?\"\\n\\nThis version is designed to elicit detailed information on the foundational theories behind both deep learning and traditional machine learning, showcase how each is applied in real-world scenarios across different sectors, and highlight the critical differences between the two methodologies.', 'documents': [], 'times_transformed': 2, 'web_search': 'Yes'}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: no\n",
      "---GRADE: DOCUMENTS NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "Node 'grade_documents':\n",
      "---TRANSFORM QUERY---\n",
      "---NEW QUESTION---\n",
      "Certainly! Here's an optimized version of the question for a web search that aims to capture the detailed information and specific distinctions you're looking for:\n",
      "\n",
      "---\n",
      "\n",
      "\"What are the fundamental principles, practical applications, and significant differences between deep learning and traditional machine learning algorithms? Please provide examples of how each approach is utilized in various industries, such as healthcare, finance, and technology.\"\n",
      "\n",
      "---\n",
      "\n",
      "This revised question is structured to yield comprehensive results that cover the theoretical underpinnings, real-world applications, and critical distinctions between deep learning and traditional machine learning, complete with industry-specific examples.\n",
      "Node 'transform_query':\n",
      "{'question': 'Certainly! Here\\'s an optimized version of the question for a web search that aims to capture the detailed information and specific distinctions you\\'re looking for:\\n\\n---\\n\\n\"What are the fundamental principles, practical applications, and significant differences between deep learning and traditional machine learning algorithms? Please provide examples of how each approach is utilized in various industries, such as healthcare, finance, and technology.\"\\n\\n---\\n\\nThis revised question is structured to yield comprehensive results that cover the theoretical underpinnings, real-world applications, and critical distinctions between deep learning and traditional machine learning, complete with industry-specific examples.', 'documents': [], 'times_transformed': 3, 'web_search': 'Yes'}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: yes\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "Node 'grade_documents':\n",
      "---GENERATE---\n",
      "Node 'generate':\n",
      " Deep learning and traditional machine learning algorithms are two distinct approaches to machine learning. Deep learning involves training artificial neural networks with multiple layers of processing units to learn complex patterns and make predictions. On the other hand, traditional machine learning algorithms, such as logistic regression and decision trees, are based on statistical models and assume a linear relationship between input features and the target variable.\n",
      "\n",
      "In healthcare, deep learning is used for medical image analysis, such as detecting cancerous tumors in medical scans. It can also be used for personalized medicine, where algorithms are trained on patient data to predict treatment outcomes. In finance, deep learning is used for fraud detection and risk management. It can analyze large amounts of data and identify patterns that may indicate fraudulent activity.\n",
      "\n",
      "In technology, deep learning is used for natural language processing, voice recognition, and autonomous vehicles. It can also be used for image and video recognition, and for generating realistic images and animations.\n",
      "\n",
      "In conclusion, deep learning and traditional machine learning algorithms are two distinct approaches to machine learning, each with its own strengths and weaknesses. Deep learning is particularly well-suited for complex tasks that require a deep understanding of the data, while traditional machine learning algorithms are better suited for tasks that involve linear relationships between input features and the target variable.\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"What is Deep learning?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        print(f\"Node '{key}':\")\n",
    "# Final generation\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceae079",
   "metadata": {},
   "source": [
    "Graph Initialization and Initial Retrieval: \n",
    "The LangGraph execution begins at the START node with the input question \"What is Deep learning?\" and immediately flows through an unconditional edge to the **set_state** node, which initializes the graph state by setting `times_transformed = `0. The execution then follows a direct edge to the **retrieve** node, which queries the Chroma vector database using Amazon Titan embeddings and successfully retrieves 4 documents from the knowledge base containing agent-related articles.\n",
    "\n",
    "Document Grading and Conditional Routing: \n",
    "The flow continues via an unconditional edge to the **grade_documents** node, where each retrieved document undergoes relevance assessment using the Amazon Titan Express LLM. *All 4 documents receive a binary score of \"no\" (irrelevant)*, causing the node to set `web_search = \"Yes` and return an empty filtered document list. This triggers the **decide_to_generate** conditional edge, which evaluates two conditions: the web_search flag status and the transformation counter. Since `web_search == \"Yes\"` and `times_transformed < 3`, the conditional logic routes execution to the \"should_transform_query\" path.\n",
    "\n",
    "Query Transformation Loop: \n",
    "The **transform_query** node executes, using the Amazon Nova Pro LLM to rewrite the original question into a more detailed, search-optimized version while `incrementing times_transformed = 1`. An unconditional edge returns execution to the **retrieve** node, creating a feedback loop. This cycle repeats three times: retrieve → grade_documents → decide_to_generate → transform_query → retrieve. During the first two iterations, all retrieved documents continue to receive \"no\" grades, but the conditional edge logic allows continued transformation since the retry limit hasn't been reached.\n",
    "\n",
    "Successful Resolution and Answer Generation:\n",
    "On the fourth retrieval attempt (after the third query transformation), the **grade_documents** node finally identifies one document as relevant (grade = \"yes\"), setting `web_search = \"No\"` and passing the filtered document forward. The **decide_to_generate** conditional edge now evaluates `web_search == \"No\"` and routes execution to the \"**should_generate**\" path, leading to the generate node. This final node uses the Titan Lite LLM with the RAG chain to create a comprehensive answer using the single relevant document as context, before flowing through an unconditional edge to the END node, successfully completing the workflow.\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    START([START Input: What is Deep learning?]) --> set_state[set_state Initialize times_transformed = 0]\n",
    "    \n",
    "    set_state --> retrieve1[retrieve Get 4 documents from vector store]\n",
    "    \n",
    "    retrieve1 --> grade1[grade_documents All 4 docs graded no web_search = Yes]\n",
    "    \n",
    "    grade1 --> decision1{decide_to_generate Check: times_transformed less than 3 web_search = Yes}\n",
    "    \n",
    "    decision1 -->|should_transform_query| transform1[transform_query Rewrite query attempt 1 times_transformed = 1]\n",
    "    \n",
    "    transform1 --> retrieve2[retrieve Get 4 new documents]\n",
    "    \n",
    "    retrieve2 --> grade2[grade_documents All 4 docs graded no web_search = Yes]\n",
    "    \n",
    "    grade2 --> decision2{decide_to_generate Check: times_transformed less than 3 web_search = Yes}\n",
    "    \n",
    "    decision2 -->|should_transform_query| transform2[transform_query Rewrite query attempt 2 times_transformed = 2]\n",
    "    \n",
    "    transform2 --> retrieve3[retrieve Get 4 new documents]\n",
    "    \n",
    "    retrieve3 --> grade3[grade_documents All 4 docs graded no web_search = Yes]\n",
    "    \n",
    "    grade3 --> decision3{decide_to_generate Check: times_transformed less than 3 web_search = Yes}\n",
    "    \n",
    "    decision3 -->|should_transform_query| transform3[transform_query Rewrite query attempt 3 times_transformed = 3]\n",
    "    \n",
    "    transform3 --> retrieve4[retrieve Get 4 new documents]\n",
    "    \n",
    "    retrieve4 --> grade4[grade_documents 1 doc graded yes web_search = No]\n",
    "    \n",
    "    grade4 --> decision4{decide_to_generate web_search = No}\n",
    "    \n",
    "    decision4 -->|should_generate| generate[generate Create final RAG answer Using 1 relevant document]\n",
    "    \n",
    "    generate --> END([END Success: Answer generated])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ae5140",
   "metadata": {},
   "source": [
    "In the above question, no document returned. But in the following question documents are retured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93f5f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---SET STATE---\n",
      "Node 'set_state':\n",
      "{'question': 'What is Short term memory?', 'times_transformed': 0}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: yes\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: yes\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: yes\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: no\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "Node 'grade_documents':\n",
      "---GENERATE---\n",
      "Node 'generate':\n",
      " Short-term memory is believed to have the capacity of about 7 items and lasts for 20-30 seconds.\n"
     ]
    }
   ],
   "source": [
    "# return 3 documents\n",
    "inputs = {\"question\": \"What is Short term memory?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        print(f\"Node '{key}':\")\n",
    "# Final generation\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817d2ed5",
   "metadata": {},
   "source": [
    "Step 1: Graph Initialization and Entry Point:\n",
    "\n",
    "Node: `START` → **set_state**\n",
    "\n",
    "Edge: Unconditional edge from `START` to **set_state**\n",
    "Execution: The LangGraph workflow begins execution with the input `{\"question\": \"What is Short term memory?\"}`. The framework automatically routes to the **set_state** node via the predefined `workflow.add_edge(START, \"set_state\")` configuration. This node initializes the critical state variable `times_transformed = 0`, which serves as a retry counter for query transformation attempts. The state is now `{'question': 'What is Short term memory?', 'times_transformed': 0}`, establishing the foundation for all subsequent node operations.\n",
    "\n",
    "\n",
    "Step 2: Document Retrieval from Vector Database\n",
    "\n",
    "Node: **set_state** → **retrieve**\n",
    "\n",
    "Edge: Unconditional edge defined by `workflow.add_edge(\"set_state\", \"retrieve\")`\n",
    "\n",
    "Execution: The workflow transitions to the `retrieve` node, which extracts the question from the shared state and invokes `retriever.get_relevant_documents(question)`. This operation queries the Chroma vector database using Amazon Titan embeddings to perform semantic similarity search against the pre-indexed agent-related articles. The retrieval successfully returns 4 document chunks, all sourced from \"https://lilianweng.github.io/posts/2023-06-23-agent/\", which are added to the state as `{\"documents\": [Document1, Document2, Document3, Document4]}`.\n",
    "\n",
    "Step 3: Document Relevance Assessment\n",
    "\n",
    "Node: **retrieve** → **grade_documents**\n",
    "\n",
    "Edge: Unconditional edge defined by `workflow.add_edge(\"retrieve\", \"grade_documents\")`\n",
    "\n",
    "Execution: The `grade_documents` node processes each retrieved document through the `retrieval_grader()` function, which uses the Amazon Titan Express LLM to evaluate relevance. For each document, it creates a prompt containing both the question and document content, then receives a binary classification (\"yes\" or \"no\"). *The grading results show: Document 1-3 receive \"Grade: yes\" (relevant), while Document 4 receives \"Grade: no\" (irrelevant)*. Since 3 documents are deemed relevant, the node sets `web_search = \"No\"` and updates the state with `{\"documents\": [Document1, Document2, Document3], \"web_search\": \"No\"}`.\n",
    "\n",
    "Step 4: Conditional Decision Logic\n",
    "\n",
    "Node: **grade_documents** → **decide_to_generate** (conditional edge)\n",
    "\n",
    "Edge: Conditional edge defined by `workflow.add_conditional_edges()` with routing logic\n",
    "\n",
    "Conditions Evaluated: The `decide_to_generate()` function examines the `web_search` flag in the current state. Since `web_search == \"No\"` (indicating relevant documents were found), the condition `if web_search == \"Yes\"` evaluates to False. The function bypasses the retry logic checks (`times_transformed >= 3`) and proceeds directly to the else clause, printing \"---DECISION: GENERATE---\" and returning the string \"**should_generate**\". This return value determines the next node via the conditional routing dictionary `{\"should_transform_query\": \"transform_query\", \"should_generate\": \"generate\"}`.\n",
    "\n",
    "Step 5: Answer Generation with RAG Chain\n",
    "\n",
    "Node: **grade_documents** → **generate**\n",
    "\n",
    "Edge: Conditional edge routing to `\"should_generate\": \"generate\"`\n",
    "\n",
    "Execution: The **generate** node receives the filtered relevant documents and the original question from the shared state. It invokes the `rag_chain.invoke()` method, which consists of a sequential pipeline: prompt formatting → Amazon Titan Lite LLM processing → string output parsing. The RAG chain combines the 3 relevant documents using `format_docs()` function and the hub-pulled \"rlm/rag-prompt\" template to create a contextual prompt. The Titan Lite model generates the response: \"Short-term memory is believed to have the capacity of about 7 items and lasts for 20-30 seconds.\" This generation is added to the state as `{\"generation\": \"...\"}`.\n",
    "\n",
    "Step 6: Workflow Termination\n",
    "\n",
    "Node: **generate** → END\n",
    "\n",
    "Edge: Unconditional edge defined by `workflow.add_edge(\"generate\", END)`\n",
    "\n",
    "Execution: The **generate** node completes successfully and the workflow follows the unconditional edge to the END node, terminating the graph execution. The final state contains all accumulated data: the original question, the 3 relevant documents, the generated answer, and metadata including `times_transformed = 0` and `web_search = \"No\"`. The `app.stream()` method yields the final output, allowing the calling code to extract and display the generated answer.\n",
    "\n",
    "Path Analysis: Direct Success Route\n",
    "\n",
    "Path Taken: START → set_state → retrieve → grade_documents → decide_to_generate → generate → END\n",
    "\n",
    "Path Not Taken: The **transform_query** node remains unexecuted because the conditional logic in `decide_to_generate()` determined that sufficient relevant documents were available. The feedback loop edge **workflow.add_edge(\"transform_query\", \"retrieve\")** was never traversed, demonstrating the efficiency of the LangGraph's adaptive routing when the initial query successfully matches the knowledge base content. This execution represents the optimal scenario where no query refinement or multiple retrieval attempts are necessary.\n",
    "\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    INPUT[🔍 Input Question<br/>What is Short term memory?]\n",
    "    \n",
    "    INIT[🚀 set_state<br/>times_transformed = 0]\n",
    "    \n",
    "    RETRIEVE[📚 retrieve<br/>Get 4 documents from vector DB]\n",
    "    \n",
    "    GRADE[🎯 grade_documents<br/>3 docs: relevant ✅<br/>1 doc: not relevant ❌]\n",
    "    \n",
    "    DECISION{🤔 decide_to_generate<br/>Check: Has relevant docs?}\n",
    "    \n",
    "    TRANSFORM[🔄 transform_query<br/>❌ NOT EXECUTED<br/>Query rewriting]\n",
    "    \n",
    "    GENERATE[💡 generate<br/>Create answer using<br/>3 relevant documents]\n",
    "    \n",
    "    ANSWER[📝 Final Answer<br/>Short-term memory capacity:<br/>~7 items, lasts 20-30 seconds]\n",
    "    \n",
    "    %% Main execution flow (Cell 29 actual path)\n",
    "    INPUT --> INIT\n",
    "    INIT --> RETRIEVE\n",
    "    RETRIEVE --> GRADE\n",
    "    GRADE --> DECISION\n",
    "    \n",
    "    %% Decision branches\n",
    "    DECISION -->|✅ YES: Has relevant docs<br/>Route to generate| GENERATE\n",
    "    DECISION -.->|❌ NO: Transform query<br/>NOT TAKEN in Cell 29| TRANSFORM\n",
    "    \n",
    "    %% Transform loop (not executed in this case)\n",
    "    TRANSFORM -.-> RETRIEVE\n",
    "    \n",
    "    %% Final result\n",
    "    GENERATE --> ANSWER\n",
    "    \n",
    "    %% Styling\n",
    "    classDef executed fill:#c8e6c9,stroke:#4caf50,stroke-width:3px\n",
    "    classDef notExecuted fill:#ffcdd2,stroke:#f44336,stroke-width:2px,stroke-dasharray: 5 5\n",
    "    classDef decision fill:#fff3e0,stroke:#ff9800,stroke-width:3px\n",
    "    classDef inputOutput fill:#e3f2fd,stroke:#2196f3,stroke-width:3px\n",
    "    \n",
    "    class INPUT,ANSWER inputOutput\n",
    "    class INIT,RETRIEVE,GRADE,GENERATE executed\n",
    "    class TRANSFORM notExecuted\n",
    "    class DECISION decision\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85dae87",
   "metadata": {},
   "source": [
    "[^1]: [What is an AI agent?](https://blog.langchain.com/what-is-an-agent/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-bedrock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
